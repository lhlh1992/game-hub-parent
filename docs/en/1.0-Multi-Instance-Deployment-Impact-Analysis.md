# 1.0 Multi-Instance Deployment Impact Analysis

> **Document Version**: 1.0  
> **Analysis Date**: 2025-12-17  
> **Scope**: All business code, all services, all features in version 1.0  
> **Goal**: Clearly identify issues under multi-instance deployment for the current version and provide corresponding upgrade plans

---

## I. Document Description

### 1.1 Scope of Analysis

This document analyzes, for **version 1.0**, the impact of multi-instance deployment on each feature:

- **Gateway service**: login/authentication, session management, token management
- **Game-Service**: room management, game state, AI execution, countdown, WebSocket broadcast
- **Chat-Service**: lobby chat, room chat, private chat, WebSocket broadcast
- **System-Service**: user management, friend relations, session monitoring
- **Common libraries**: SessionRegistry, Kafka events

**Note**: This document only contains features that **are affected**. Features without impact under multi-instance deployment are excluded.

### 1.2 Analysis Dimensions

For each feature, we analyze the following dimensions:

1. **Function Description**: what the current implementation does
2. **Multi-Instance Impact**: what happens under multi-instance deployment
3. **Root Cause**: technical reason behind the impact
4. **Impact Level**: üî¥ Critical / üü† High / üü° Medium / üü¢ Low
5. **Upgrade Plan**: upgrade plan under the constraint of *not changing the overall architecture*

---

## II. Overall Problem Overview

From the current implementation, the core issues under multi-instance deployment can be roughly summarized into four categories; most detailed items later in this document can be mapped to one or more of these categories:

1. **Room Affinity Problem**  
   - HTTP / WebSocket requests for the **same room** may be routed to **different instances** under load balancing, while the business code implicitly assumes ‚Äúthe same room stays on the same instance for a long time‚Äù.  
   - Typical impact: AI delayed moves can be scheduled on multiple instances at the same time; countdown restore runs on multiple instances simultaneously; SimpleBroker broadcasts only reach connections on the current instance, players on other instances cannot see them.  
   - Upgrade path: implement room affinity routing in Gateway based on `roomId`; replace the in-memory `SimpleBroker` with an external broker (such as RabbitMQ StompRelay / Redis PubSub) for cross-instance broadcasting.

2. **In-Memory State Not Shared Across Instances**  
   - Key states such as `rooms`, `pendingAi`, local caches, and state held by scheduled tasks are kept in **each instance's own JVM memory**, instead of being centralized in Redis or synchronized via events.  
   - Under multi-instance deployment, these variables are **not shared and not consistent**, causing: different room states for the same room on instances A and B; AI scheduled twice; countdown tasks duplicated or missing.

3. **Idempotency and Retry Issues (Idempotency & Retry)**  
   - Currently, only a few critical points implement idempotency (e.g., move CAS, private chat `clientOpId`); overall, **HTTP / WebSocket / Kafka do not follow a unified idempotency design**.  
   - Under network jitter, retries, reconnects, and multi-instance concurrency, this may lead to: duplicate room creation / duplicate friend requests; repeated WS commands causing state flapping; Kafka events re-consumed and future non-idempotent logic (stats/rewards) executed multiple times.

4. **WebSocket Broadcast Single-Instance Isolation (SimpleBroker Limitations)**  
   - Both game-service and chat-service use `enableSimpleBroker("/topic", "/queue")`; `SimpleBroker` manages subscriptions and broadcasts **only inside a single instance**.  
   - As a result, a `/topic/...` message is sent only to ‚Äúsubscribers connected to this instance‚Äù, and **subscribers connected to other instances cannot receive it**:  
     - Lobby `/topic/chat.lobby` becomes ‚Äúone small lobby per instance‚Äù under multi-instance;  
     - Room `/topic/room.{roomId}`: if room members are connected to different instances, they cannot see each other's messages.  
   - Future versions need to upgrade STOMP broadcasting from in-memory `SimpleBroker` to an external message broker (such as RabbitMQ StompRelay / Redis PubSub) to achieve real cross-instance broadcasting.

All subsequent ‚Äúmulti-instance impact‚Äù sections can be mapped back to one or more of the above categories:  
Either **non-affine routing** causing ‚Äúsame room spread across multiple instances‚Äù, or **key state only in memory instead of a unified Redis/event source**, or **lack of unified idempotency protection under retry/concurrency scenarios**, plus the **SimpleBroker single-instance broadcast limitation**.

---

## III. Gateway Service Multi-Instance Impact Analysis

### 2.1 Login / Authentication

#### Function Description
- **Location**: `LoginSessionKickHandler.onAuthenticationSuccess()`
- **Function**: after successful login, register session into SessionRegistry, kick old sessions, write blacklist, send Kafka event

#### Multi-Instance Impact
- **Impact**: when two instances process the same user's login request concurrently, both may register successfully, breaking single-device-login
- **Impact Level**: üü† **High**

#### Root Cause
- `SessionRegistry.registerLoginSessionEnforceSingle()` is not fully atomic
- Two instances query ACTIVE sessions at the same time (both may see "empty"), then both register a new session
- Although SETNX is used, there is a window between querying and registration

#### Upgrade Plan
```java
// Add a distributed lock before registerLoginSessionEnforceSingle()
String lockKey = "session:register:" + userId;
Boolean acquired = redis.opsForValue().setIfAbsent(lockKey, instanceId, Duration.ofSeconds(5));
if (!Boolean.TRUE.equals(acquired)) {
    // Wait 100ms and retry
    Thread.sleep(100);
    return registerLoginSessionEnforceSingle(...); // recursive retry
}
try {
    // Execute registration logic
} finally {
    redis.delete(lockKey);
}
```

---

### 2.2 Session Management (SessionRegistry)

#### Function Description
- **Location**: `SessionRegistry.registerLoginSessionEnforceSingle()`
- **Function**: register login session, mark old sessions as KICKED, return the list of kicked sessions

#### Multi-Instance Impact
- **Impact**: under concurrent registration, both instances may think ‚Äúno ACTIVE session exists‚Äù, both register successfully
- **Impact Level**: üü† **High**

#### Root Cause
- Flow "query ACTIVE sessions ‚Üí mark old sessions ‚Üí register new session" is not atomic
- When two instances run almost at the same time, during the query phase each may not see the other one's session

#### Upgrade Plan
- Same as 2.1: add a distributed lock

---

### 2.3 Token Fetch API

#### Function Description
- **Location**: `TokenController.getToken()`
- **Function**: read `access_token` from HTTP Session and return it to the frontend

#### Multi-Instance Impact
- **Impact**: if the user logged in via instance 1 but subsequent requests are routed to instance 2, token may not be found (if Session is not shared)
- **Impact Level**: üü° **Medium**

#### Root Cause
- Spring WebFlux Session is by default stored in memory and **not shared** across instances
- Without session affinity at the Gateway layer, a user may connect to different instances later

#### Upgrade Plan
- **Option 1**: store Session in Redis (recommended)
  ```yaml
  spring:
    session:
      store-type: redis
  ```
- **Option 2**: implement session affinity at Gateway (route based on userId)

---

## IV. Game-Service Multi-Instance Impact Analysis

### 3.1 Room Creation

#### Function Description
- **Location**: `GomokuRestController.newRoom()`
- **Function**: create a new room, persist room meta info to Redis, and add it into the room index (ZSET)

#### Multi-Instance Impact
- **Impact**: write contention on a single ZSET key; does not cause data errors, but decreases performance
- **Impact Level**: üü° **Medium**

#### Root Cause
- All room indexes are stored in one ZSET key `gomoku:rooms:index`
- When multiple instances create rooms concurrently, they write the same key; Redis serializes these writes internally, increasing latency

#### Upgrade Plan
```java
// Sharding: hash roomId into multiple buckets
public static String roomIndexKey(String roomId) {
    int bucket = Math.abs(roomId.hashCode()) % 100;
    return "gomoku:rooms:index:" + bucket;
}
```

---

### 3.2 In-Memory Room Cache (Inconsistency Across Instances)

#### Function Description
- **Location**: `GomokuServiceImpl.rooms`
- **Function**: in-memory cache of `Room` objects to avoid frequent Redis loads

#### Multi-Instance Impact
- **Impact**: under multi-instance deployment, each instance has its own `rooms` map; the `Room` object for the same room may differ across instances
- **Impact Level**: üü† **High**

#### Root Cause
- `GomokuServiceImpl.rooms` is an in-memory `ConcurrentHashMap`, independent per instance
- Method `room()`:
  - first reads from memory; if absent, loads from Redis and caches it
- Under multi-instance, instance 1 and instance 2 can both cache the same room into their own `rooms`
- When instance 1 modifies the Room (e.g., seat binding), instance 2's in-memory Room is not updated
- Room also contains several internal maps (`seatBySession`, `seatKeyToSeat`, `seatToSessionId`), which can diverge across instances

#### Upgrade Plan
```java
// Option 1: always reload from Redis before each operation (safest but less performant)
private Room room(String roomId) {
    // Always load from Redis, no cache
    RoomMeta meta = roomRepo.getRoomMeta(roomId)...;
    Room r = RoomMetaConverter.toRoom(meta);
    // ... backfill seat bindings and game state ...
    return r;
}

// Option 2: cache + version (recommended)
// Add a version field in RoomMeta, increment version on each update
private Room room(String roomId) {
    Room cached = rooms.get(roomId);
    if (cached != null) {
        RoomMeta meta = roomRepo.getRoomMeta(roomId).orElse(null);
        if (meta != null && meta.getVersion() == cached.getVersion()) {
            return cached; // version matches, safe to use cache
        }
        // version mismatch, refresh
        rooms.remove(roomId);
    }
    // load from Redis and cache
    // ...
}

// Option 3: room affinity (simplest, but requires Gateway support)
// Route all operations for the same room to the same instance to avoid cross-instance concurrency
```

---

### 3.3 Player Move (Concurrency Control)

#### Function Description
- **Location**: `GomokuWsController.place()`
- **Function**: handle player moves, update game state, CAS update Redis

#### Multi-Instance Impact
- **Impact**: higher CAS failure rate (but no data corruption); memory state may be dirty after CAS failure; with in-memory Room cache, logic may run based on dirty state
- **Impact Level**: üü† **High** (dirty data after CAS failure + Room cache inconsistency)

#### Root Cause
- Flow: mutate Room in memory ‚Üí CAS update in Redis
- If CAS fails, memory state has been changed but Redis not updated; subsequent logic (e.g., AI decision) may use dirty state
- With in-memory Room cache, different instances may base decisions on different versions of the state

#### Upgrade Plan
```java
// After CAS failure, immediately reload from Redis and refresh memory
boolean success = gameStateRepository.updateAtomically(...);
if (!success) {
    // Clear in-memory Room cache so next time loads from Redis
    rooms.remove(roomId);
    GomokuState freshState = gameStateRepository.load(roomId);
    gomokuService.refreshState(roomId, freshState);
    throw new ConcurrentModificationException("Concurrent conflict, please retry");
}
```

---

### 3.4 AI Execution (PVE Mode)

#### Function Description
- **Location**: `GomokuWsController.maybeScheduleAi()`, `runAiTurn()`
- **Function**: in PVE mode, when it is AI's turn, schedule AI move with a delay

#### Multi-Instance Impact
- **Impact**: AI for the same room may run concurrently on multiple instances, causing duplicate AI moves and chaotic game state
- **Impact Level**: üî¥ **Critical**

#### Root Cause
- `pendingAi` is an in-memory variable (`ConcurrentHashMap`), independent per instance
- Under multi-instance deployment, players of the same room may connect to different instances
- Both instances can decide ‚Äúit's AI's turn‚Äù and schedule their own AI task

#### Upgrade Plan
```java
// Add a distributed lock before maybeScheduleAi()
String lockKey = "ai:lock:" + roomId;
Boolean acquired = redis.opsForValue().setIfAbsent(lockKey, instanceId, Duration.ofSeconds(5));
if (!Boolean.TRUE.equals(acquired)) {
    log.debug("AI task is already running on another instance, skip: roomId={}", roomId);
    return;
}
try {
    // Execute AI logic
} finally {
    redis.delete(lockKey);
}
```

---

### 3.5 Countdown Restore (Service Startup)

#### Function Description
- **Location**: `CountdownSchedulerImpl.restoreAllActive()`
- **Function**: restore all unexpired countdown tasks on service startup

#### Multi-Instance Impact
- **Impact**: when multiple instances start at the same time, each instance restores all countdowns, effectively "accelerating" countdown (e.g., 30 seconds becomes 10 seconds)
- **Impact Level**: üî¥ **Critical**

#### Root Cause
- Each instance calls `restoreAllActive()` on `ApplicationReadyEvent`
- Each instance scans `countdown:*` and restores all countdowns
- The same countdown is scheduled by multiple instances simultaneously

#### Upgrade Plan
```java
// Add a distributed lock before restoreAllActive()
String lockKey = "countdown:restore:lock";
Boolean acquired = redis.opsForValue().setIfAbsent(lockKey, nodeId, Duration.ofSeconds(30));
if (!Boolean.TRUE.equals(acquired)) {
    log.info("Another instance is restoring countdowns, skip");
    return 0;
}
try {
    // Execute restore logic
} finally {
    redis.delete(lockKey);
}
```

---

### 3.6 Countdown Restore (Redis KEYS Blocking)

#### Function Description
- **Location**: `CountdownSchedulerImpl.restoreAllActive()`
- **Function**: use `redis.keys("countdown:*")` to scan all countdown keys

#### Multi-Instance Impact
- **Impact**: when multiple instances start together and each executes KEYS, Redis main thread is blocked; all requests suffer latency spikes
- **Impact Level**: üî¥ **Critical**

#### Root Cause
- `KEYS` is a blocking full keyspace scan; with many keys it blocks Redis main thread
- When multiple instances call KEYS at the same time, Redis is flooded with blocking commands

#### Upgrade Plan
```java
// Replace KEYS with SCAN
Set<String> keys = new HashSet<>();
String cursor = "0";
do {
    ScanOptions options = ScanOptions.scanOptions()
        .match(stateKey("*"))
        .count(100)
        .build();
    Cursor<String> scanCursor = redis.scan(options);
    while (scanCursor.hasNext()) {
        keys.add(scanCursor.next());
    }
    cursor = scanCursor.getCursor();
} while (!"0".equals(cursor));
```

---

### 3.7 WebSocket Broadcast (SimpleBroker)

#### Function Description
- **Location**: `WebSocketStompConfig.configureMessageBroker()`
- **Function**: use SimpleBroker as in-memory message broker to broadcast to `/topic/room.{roomId}`

#### Multi-Instance Impact
- **Impact**: if players of a room connect to different instances, broadcast fails (players cannot see each other's moves)
- **Impact Level**: üî¥ **Critical**

#### Root Cause
- SimpleBroker is an in-memory broker that can only broadcast inside a single instance
- If player 1 connects to instance 1 and player 2 to instance 2, broadcasts from instance 1 are only received by subscribers on instance 1

#### Upgrade Plan
- **Option 1**: room affinity (smaller change)
  ```java
  // At Gateway layer, route by roomId with consistent hashing
  String targetInstance = consistentHash(roomId, availableInstances);
  ```
- **Option 2**: external broker (Redis PubSub or RabbitMQ)
  ```java
  registry.enableStompBrokerRelay("/topic", "/queue")
      .setRelayHost("redis-host")
      .setRelayPort(6379);
  ```

---

### 3.8 Room Join

#### Function Description
- **Location**: `GomokuRestController.joinRoom()`
- **Function**: player joins a room, auto-assigns seat, broadcasts SNAPSHOT

#### Multi-Instance Impact
- **Impact**: if players of the room connect to different instances, SNAPSHOT broadcast may fail (SimpleBroker limitation)
- **Impact Level**: üî¥ **Critical** (broadcast failure)

#### Root Cause
- `broadcastSnapshot()` uses SimpleBroker and can only broadcast within a single instance
- If room members connect to different instances, some players will not receive the SNAPSHOT

#### Upgrade Plan
- Same as 3.7: use external broker or room affinity

---

### 3.9 Room Leave

#### Function Description
- **Location**: `GomokuRestController.leaveRoom()`
- **Function**: player leaves room, clean seat bindings, broadcast SNAPSHOT

#### Multi-Instance Impact
- **Impact**: if players of the room connect to different instances, SNAPSHOT broadcast may fail (SimpleBroker limitation)
- **Impact Level**: üî¥ **Critical** (broadcast failure)

#### Root Cause
- `broadcastSnapshot()` uses SimpleBroker and can only broadcast within a single instance
- If room members connect to different instances, some players will not receive the SNAPSHOT

#### Upgrade Plan
- Same as 3.7: use external broker or room affinity

---

### 3.10 Seat Binding (`seatKey`)

#### Function Description
- **Location**: `RedisRoomRepository.setSeatKey()`, `GomokuResumeController.onResume()`
- **Function**: bind seat by `seatKey` (for reconnect resume)

#### Multi-Instance Impact
- **Impact**: `setSeatKey()` is fine (Redis SETNX is atomic); `deleteSeatKeys()` uses KEYS and can block Redis
- **Impact Level**: üü° **Medium** (blocking risk in `deleteSeatKeys`)

#### Root Cause
- `setSeatKey()` uses Redis SETNX, atomic, safe under multi-instance
- `deleteSeatKeys()` uses `redis.keys()`, a blocking operation; multiple instances calling it will block Redis

#### Upgrade Plan
```java
// Change deleteSeatKeys() to use SCAN
Set<String> keys = new HashSet<>();
String cursor = "0";
String pattern = RedisKeys.roomSeatKeyPrefix(roomId) + "*";
do {
    ScanOptions options = ScanOptions.scanOptions()
        .match(pattern)
        .count(100)
        .build();
    Cursor<String> scanCursor = redis.scan(options);
    while (scanCursor.hasNext()) {
        keys.add(scanCursor.next());
    }
    cursor = scanCursor.getCursor();
} while (!"0".equals(cursor));
if (!keys.isEmpty()) {
    redisTemplate.delete(keys);
}
```

---

### 3.11 Reconnect Resume

#### Function Description
- **Location**: `GomokuResumeController.onResume()`
- **Function**: on client reconnect, restore seat by `seatKey` and return full snapshot

#### Multi-Instance Impact
- **Impact**: if reconnects go to a different instance, previous WebSocket Session may not be found (if Session is not shared)
- **Impact Level**: üü° **Medium**

#### Root Cause
- WebSocket Session is stored in memory by default and not shared across instances
- If user reconnects to a different instance, previous Session info is lost (though `seatKey` in Redis still allows seat recovery)

#### Upgrade Plan
- If Session info must be shared across instances, store WebSocket sessions in Redis; otherwise current design (using `seatKey`) is acceptable

---

### 3.12 Resign

#### Function Description
- **Location**: `GomokuWsController.resign()`
- **Function**: player resigns, updates game state, broadcasts result

#### Multi-Instance Impact
- **Impact**: same as 3.3, CAS failure may lead to dirty data; broadcast may fail
- **Impact Level**: üü† **High** (CAS failure) + üî¥ **Critical** (broadcast failure)

#### Root Cause
- Same as 3.3 (dirty data after CAS failure)
- Same as 3.7 (SimpleBroker broadcast failure)

#### Upgrade Plan
- Same as 3.3 (CAS failure reload) + same as 3.7 (external broker)

---

### 3.13 Ready / Start / Restart

#### Function Description
- **Location**: `GomokuWsController.ready()`, `start()`, `restart()`
- **Function**: player ready, start game, restart game; update room state, broadcast SNAPSHOT

#### Multi-Instance Impact
- **Impact**: broadcast may fail (SimpleBroker limitation)
- **Impact Level**: üî¥ **Critical** (broadcast failure)

#### Root Cause
- Same as 3.7

#### Upgrade Plan
- Same as 3.7

---

### 3.14 Kick Player

#### Function Description
- **Location**: `GomokuWsController.kick()`
- **Function**: room owner kicks other players, cleans seat bindings, broadcasts SNAPSHOT

#### Multi-Instance Impact
- **Impact**: broadcast may fail (SimpleBroker limitation)
- **Impact Level**: üî¥ **Critical** (broadcast failure)

#### Root Cause
- Same as 3.7

#### Upgrade Plan
- Same as 3.7

---

### 3.15 Countdown TICK Event

#### Function Description
- **Location**: `TurnClockCoordinator.onReady()`, `CountdownScheduler.tickListener`
- **Function**: trigger TICK events every second and broadcast remaining countdown time

#### Multi-Instance Impact
- **Impact**: if players of a room connect to different instances, they may not receive TICK events (SimpleBroker limitation)
- **Impact Level**: üî¥ **Critical** (broadcast failure)

#### Root Cause
- TICK events are broadcast via SimpleBroker to `/topic/room.{roomId}`
- Same as 3.7: SimpleBroker cannot broadcast across instances

#### Upgrade Plan
- Same as 3.7

---

### 3.16 Countdown TIMEOUT Event

#### Function Description
- **Location**: `TurnClockCoordinator.handleTimeout()`
- **Function**: when countdown times out, decide loss and broadcast TIMEOUT event

#### Multi-Instance Impact
- **Impact**: broadcast may fail (SimpleBroker limitation); timeout handling itself is protected by holder lock and will not be executed twice
- **Impact Level**: üî¥ **Critical** (broadcast failure)

#### Root Cause
- Timeout handling uses a holder lock (`tryAcquireHolder()`), so it will not run twice
- But broadcast uses SimpleBroker and may fail across instances

#### Upgrade Plan
- Same as 3.7

---

### 3.17 WebSocket Connection Management (Connect)

#### Function Description
- **Location**: `WebSocketSessionManager.handleSessionConnect()`
- **Function**: when WebSocket connection is established, register session into SessionRegistry and kick old connections for the same service

#### Multi-Instance Impact
- **Impact**: if a user establishes connections on two instances at the same time, both may be registered (if query and registration are not atomic)
- **Impact Level**: üü° **Medium**

#### Root Cause
- Flow "query old connections ‚Üí delete old connections ‚Üí register new connection" is not atomic
- When two instances process connections nearly at the same time, both may think ‚Äúno old connection exists‚Äù and register successfully

#### Upgrade Plan
```java
// Add a distributed lock before registration
String lockKey = "ws:register:" + userId + ":game-service";
Boolean acquired = redis.opsForValue().setIfAbsent(lockKey, instanceId, Duration.ofSeconds(5));
if (!Boolean.TRUE.equals(acquired)) {
    // Another instance is registering, wait and retry
    Thread.sleep(100);
    // Re-query and register
}
try {
    // Execute registration logic
} finally {
    redis.delete(lockKey);
}
```

---

### 3.18 Session Invalidated Listener (Kafka Consumer)

#### Function Description
- **Location**: `SessionInvalidatedListener.onSessionInvalidated()`
- **Function**: consume Kafka session-invalidated events and close user's WebSocket connections

#### Multi-Instance Impact
- **Impact**: if the message is consumed multiple times, disconnect may also be executed multiple times (disconnect is idempotent)
- **Impact Level**: üü° **Medium**

#### Root Cause
- Kafka may deliver duplicate messages
- There is currently no de-dup mechanism; duplicate consumption simply repeats disconnect logic

#### Upgrade Plan
- Same as 6.2: add de-duplication

---

## V. Chat-Service Multi-Instance Impact Analysis

### 4.1 Lobby Chat

#### Function Description
- **Location**: `ChatWsController.sendLobby()`, `ChatMessagingService.sendLobbyMessage()`
- **Function**: send lobby message, broadcast to `/topic/chat.lobby`

#### Multi-Instance Impact
- **Impact**: if users connect to different instances, some may not receive the message (SimpleBroker limitation)
- **Impact Level**: üî¥ **Critical**

#### Root Cause
- Uses SimpleBroker, which can broadcast only within a single instance
- If user A connects to instance 1 and user B to instance 2, messages from user A will be received only by subscribers on instance 1

#### Upgrade Plan
- Switch to an external broker (Redis PubSub or RabbitMQ), **must** use an external broker (lobby chat is not suitable for room affinity only)

---

### 4.2 Room Chat

#### Function Description
- **Location**: `ChatWsController.sendRoom()`, `ChatMessagingService.sendRoomMessage()`
- **Function**: send room chat message, broadcast to `/topic/chat.room.{roomId}` and store in Redis List

#### Multi-Instance Impact
- **Impact**: if players of the room connect to different instances, some may not receive the message (SimpleBroker limitation)
- **Impact Level**: üî¥ **Critical**

#### Root Cause
- Same as 4.1: SimpleBroker limitation

#### Upgrade Plan
- Switch to an external broker or use room affinity

---

### 4.3 Private Message

#### Function Description
- **Location**: `ChatWsController.sendPrivate()`, `ChatMessagingService.sendPrivateMessage()`
- **Function**: send private message, push via point-to-point destination (`/user/queue/chat.private`), persist to PostgreSQL

#### Multi-Instance Impact
- **Impact**: if the target user connects to a different instance, message delivery may fail (SimpleBroker limitation)
- **Impact Level**: üî¥ **Critical**

#### Root Cause
- `convertAndSendToUser()` relies on SimpleBroker and can only route within a single instance
- If the target user is on instance 2 but the message is sent from instance 1, SimpleBroker on instance 1 cannot find the user on instance 2

#### Upgrade Plan
- Switch to an external broker (Redis PubSub or RabbitMQ) that supports cross-instance user routing

---

### 4.4 Private Session Creation

#### Function Description
- **Location**: `ChatSessionService.getOrCreatePrivateSession()`
- **Function**: get or create private chat session (DB operations)

#### Multi-Instance Impact
- **Impact**: duplicate sessions may be created if two instances create a session for the same user pair concurrently
- **Impact Level**: üü° **Medium**

#### Root Cause
- Flow "query session ‚Üí create if not exists" is not atomic
- When two instances run almost simultaneously, both may think "no session" and both create a session

#### Upgrade Plan
```java
// Use DB unique constraint (userId1, userId2) to prevent duplicates
// Or add a distributed lock in the application layer
String lockKey = "chat:session:create:" + min(userId1, userId2) + ":" + max(userId1, userId2);
```

---

### 4.5 Friend Relationship Check

#### Function Description
- **Location**: `ChatMessagingServiceImpl.sendPrivateMessage()` line 107
- **Function**: before sending a private message, use Feign to call system-service to verify friend relationship

#### Multi-Instance Impact
- **Impact**: if the Feign call fails with 401, current logic ‚Äúdegrades to allow sending‚Äù, which is a security risk
- **Impact Level**: üü† **High** (security risk)

#### Root Cause
- Current code: when catching `FeignException.Unauthorized`, logs a warning but still sends the message
- Under multi-instance, if system-service is unavailable, all instances degrade to ‚Äúallow send‚Äù, magnifying the security risk

#### Upgrade Plan
```java
// Change to security-first: if verification fails, reject send
catch (FeignException.Unauthorized e) {
    log.error("Token invalid when verifying friendship: senderId={}, targetUserId={}", senderId, targetUserId);
    return false; // reject sending
}
```

---

### 4.6 WebSocket Connection Management (Connect)

#### Function Description
- **Location**: `WebSocketSessionManager.handleSessionConnect()`
- **Function**: when WebSocket connection is established, register session into SessionRegistry and kick old connections for the same service

#### Multi-Instance Impact
- **Impact**: if the user establishes connections on two instances at the same time, both may be registered (if query and registration are not atomic)
- **Impact Level**: üü° **Medium**

#### Root Cause
- Flow "query old connections ‚Üí delete old connections ‚Üí register new connection" is not atomic
- When two instances handle connections simultaneously, both may think "no old connection" and both register successfully

#### Upgrade Plan
- Same as 3.17: add distributed lock

---

### 4.7 Session Invalidated Listener (Kafka Consumer)

#### Function Description
- **Location**: `SessionInvalidatedListener.onSessionInvalidated()`
- **Function**: consume Kafka session-invalidated events and close user's WebSocket connections

#### Multi-Instance Impact
- **Impact**: if the message is consumed multiple times, disconnect may be executed multiple times (disconnect is idempotent)
- **Impact Level**: üü° **Medium**

#### Root Cause
- Kafka may deliver duplicate messages
- There is currently no de-dup mechanism; duplicate consumption repeats disconnect logic

#### Upgrade Plan
- Same as 6.2: add de-duplication

---

### 4.8 WebSocket Token Storage (Degrade Scenario)

#### Function Description
- **Location**: `WebSocketTokenStore.fallbackStore`
- **Function**: WebSocket token storage; prefers Redis, degrades to in-memory `fallbackStore` when Redis is unavailable

#### Multi-Instance Impact
- **Impact**: under normal conditions with Redis, no impact; when Redis is unavailable and it degrades to in-memory storage, tokens are not shared across instances
- **Impact Level**: üü° **Medium** (only when Redis is unavailable)

#### Root Cause
- `fallbackStore` is an in-memory `ConcurrentHashMap`, independent per instance
- If Redis is unavailable, tokens stored by instance 1 are invisible to instance 2

#### Upgrade Plan
- **Option 1**: ensure Redis high availability and **avoid degrading to in-memory storage** (recommended)
- **Option 2**: if degradation must be supported, restrict deployment to single instance in degrade mode, or switch to another shared storage (e.g., DB)

---

## VI. System-Service Multi-Instance Impact Analysis

### 5.1 User Sync

#### Function Description
- **Location**: `UserController.syncUser()`
- **Function**: sync user info from Keycloak to system DB

#### Multi-Instance Impact
- **Impact**: duplicate sync may happen if two instances process sync for the same user concurrently
- **Impact Level**: üü° **Medium**

#### Root Cause
- Flow "query user ‚Üí create if not exists" is not atomic
- When two instances run almost at the same time, both may think "user does not exist" and both create it

#### Upgrade Plan
```java
// Use DB unique constraint (keycloak_user_id) to prevent duplicates
// Or add a distributed lock in the application layer
String lockKey = "user:sync:" + keycloakUserId;
```

---

### 5.2 Friend Management

#### Function Description
- **Location**: `FriendController.applyFriend()`, `accept()`, `reject()`
- **Function**: apply friend, accept/reject friend requests (DB operations)

#### Multi-Instance Impact
- **Impact**: without DB unique constraints, duplicate requests may be created
- **Impact Level**: üü° **Medium**

#### Root Cause
- Depends on DB schema; if there is no unique constraint like `(requester_id, receiver_id)`, two instances concurrently processing may create duplicate requests

#### Upgrade Plan
```java
// Use DB unique constraint (requester_id, receiver_id) to prevent duplicates
// Or add a distributed lock in the application layer
String lockKey = "friend:apply:" + min(userId1, userId2) + ":" + max(userId1, userId2);
```

---

### 5.3 Session Monitoring

#### Function Description
- **Location**: `SessionMonitorController.getAllSessions()`
- **Function**: query all online sessions from SessionRegistry

#### Multi-Instance Impact
- **Impact**: if SessionRegistry uses `redis.keys()` for queries, multiple instances calling it will block Redis
- **Impact Level**: üü° **Medium**

#### Root Cause
- Need to inspect `SessionRegistry.getUserIds()` implementation; if it uses KEYS, there is blocking risk

#### Upgrade Plan
- If KEYS is used, switch to SCAN

---

## VII. Common Libraries Multi-Instance Impact Analysis

### 6.1 SessionRegistry Query (KEYS Blocking)

#### Function Description
- **Location**: `SessionRegistry.getUserIds()` (line 591)
- **Function**: get all user IDs that have sessions (used by `getAllUserSessions()` and `getAllUsersWithSessions()`)

#### Multi-Instance Impact
- **Impact**: using `redis.keys()` for query; multiple instances calling it will block Redis
- **Impact Level**: üü° **Medium**

#### Root Cause
- `getUserIds()` uses `redis.keys(prefix + "*")`, a blocking operation
- With many online users (e.g., 100k), KEYS can block Redis for several seconds
- Multiple instances calling it (e.g., monitoring interface) amplifies the blocking

#### Upgrade Plan
```java
// Switch to SCAN
private Set<String> getUserIds(String prefix) {
    Set<String> userIds = new HashSet<>();
    String cursor = "0";
    do {
        ScanOptions options = ScanOptions.scanOptions()
            .match(prefix + "*")
            .count(100)
            .build();
        Cursor<String> scanCursor = redis.scan(options);
        while (scanCursor.hasNext()) {
            String key = scanCursor.next();
            userIds.add(key.substring(prefix.length()));
        }
        cursor = scanCursor.getCursor();
    } while (!"0".equals(cursor));
    return userIds;
}
```

---

### 6.2 Kafka Event Consumption

#### Function Description
- **Location**: `SessionEventConsumer.onMessage()`, `SessionInvalidatedListener.onSessionInvalidated()`
- **Function**: consume session-invalidated events and close WebSocket connections

#### Multi-Instance Impact
- **Impact**: if messages are delivered multiple times, disconnect will run multiple times (disconnect is idempotent)
- **Impact Level**: üü° **Medium**

#### Root Cause
- Kafka may redeliver messages
- There is no explicit de-duplication; duplicate consumption leads to repeated disconnect logic

#### Upgrade Plan
```java
// Add de-duplication: record processed eventId in Redis SET
String eventId = userId + ":" + loginSessionId + ":" + timestamp;
Boolean processed = redis.opsForValue().setIfAbsent("event:processed:" + eventId, "1", Duration.ofMinutes(5));
if (!Boolean.TRUE.equals(processed)) {
    log.debug("Event already processed, skip: eventId={}", eventId);
    return;
}
// Execute disconnect logic
```

---

## VIII. Impact Summary and Priorities

### 7.1 Critical Issues (üî¥ P0, Must Fix)

| Feature | Service | Impact | Upgrade Plan |
|--------|---------|--------|-------------|
| AI execution | game-service | duplicate execution under multi-instance | distributed lock |
| Countdown restore | game-service | duplicate restore under multi-instance | distributed lock + SCAN |
| Redis KEYS (countdown) | game-service | Redis blocked under multi-instance | switch to SCAN |
| Redis KEYS (seatKey) | game-service | Redis blocked under multi-instance | switch to SCAN |
| Redis KEYS (SessionRegistry) | common | Redis blocked under multi-instance | switch to SCAN |
| WebSocket broadcast (game) | game-service | SimpleBroker cannot cross instances | external broker or room affinity |
| WebSocket broadcast (room operations) | game-service | SimpleBroker cannot cross instances | external broker or room affinity |
| WebSocket broadcast (countdown TICK) | game-service | SimpleBroker cannot cross instances | external broker or room affinity |
| Lobby chat broadcast | chat-service | SimpleBroker cannot cross instances | external broker |
| Room chat broadcast | chat-service | SimpleBroker cannot cross instances | external broker |
| Private message push | chat-service | SimpleBroker cannot cross instances | external broker |

### 7.2 High Issues (üü† P1, Fix Before Production)

| Feature | Service | Impact | Upgrade Plan |
|--------|---------|--------|-------------|
| Login session registration | gateway | concurrent registration breaks single-device login | distributed lock |
| In-memory Room cache | game-service | cache inconsistency across instances | cache version or room affinity |
| Dirty data after CAS failure | game-service | memory state inconsistent | reload after CAS failure |
| Friend relationship check degradation | chat-service | security risk | change to security-first |

### 7.3 Medium Issues (üü° P2, Performance Optimization)

| Feature | Service | Impact | Upgrade Plan |
|--------|---------|--------|-------------|
| Room index hotspot | game-service | write contention | sharding |
| Token Session | gateway | session not shared | store Session in Redis |
| Private session creation | chat-service | duplicate sessions | DB unique constraint |
| User sync | system-service | duplicate sync | DB unique constraint |
| Friend addition | system-service | duplicate friend entries | DB unique constraint |
| WebSocket connection management | game-service / chat-service | concurrent registration | distributed lock |
| Session invalidated listener | game-service / chat-service | Kafka duplicate consumption | de-dup mechanism |

---

## IX. Upgrade Plan Recommendations

### 8.1 Phase 1: P0 Critical Issues (2‚Äì3 days)

**Must fix; multi-instance deployment is unusable otherwise.**

1. **AI duplicate execution** ‚Üí distributed lock (1 day)
2. **Countdown duplicate restore** ‚Üí distributed lock + SCAN (0.5 day)
3. **SimpleBroker cross-instance** ‚Üí external broker or room affinity (1‚Äì1.5 days)

### 8.2 Phase 2: P1 High Issues (1 day)

**Must fix before production.**

4. **Login session registration** ‚Üí distributed lock (0.5 day)
5. **CAS failure reload** ‚Üí reload logic (0.5 day)
6. **Friend relationship check** ‚Üí security-first (0.5 day)

### 8.3 Phase 3: P2 Medium Issues (1‚Äì2 days)

**Performance optimization, can be postponed.**

7. **Room index sharding** ‚Üí sharding logic (0.5 day)
8. **Session storage** ‚Üí Redis Session (0.5 day)
9. **DB unique constraints** ‚Üí add constraints (0.5 day)
10. **Kafka de-duplication** ‚Üí de-dup mechanism (0.5 day)

### 8.4 API Idempotency and Retry Strategy (HTTP / WS / Kafka)

- **HTTP / WS user operation idempotency**:  
  - Game moves rely on Redis CAS (`expectedStep + expectedTurn`) to ensure **state idempotency** (the same move succeeds only once);  
  - For create/mutate operations such as room creation, friend request, ready/start/kick, the current implementation mainly depends on state checks and frontend debouncing; going forward we can standardize reading `X-Idempotency-Key` / `requestId` from the frontend in Gateway Filters / Spring interceptors and use Redis `SETNX` to record ‚Äúprocessed requests‚Äù, achieving **request-level idempotency** and avoiding duplicate submissions under cross-instance retries.  
- **Kafka session event idempotency**:  
  - Producer side already enables idempotent production (`enable.idempotence=true` + `acks=all`), ensuring retries do not create duplicates;  
  - Consumer side currently relies on ‚Äúdisconnect WS is idempotent‚Äù; repeated executions do not break logic but waste resources; before production we recommend adding an `eventId` field and recording processed events in Redis to achieve true **consumer-side idempotency**.

---

## X. Summary

### 9.1 Current Status

**Multi-instance deployment status of version 1.0**: ‚ùå **Not suitable for multi-instance deployment**

**Number of critical issues**: 11 (P0)

**Issues that must be fixed before multi-instance deployment**: all 11 P0 issues

### 9.2 Status After Fix

**After fixing P0 issues**: ‚úÖ **Can be deployed in multi-instance mode** (all 11 P0 issues must be fixed)

**After fixing both P0 and P1 issues**: ‚úÖ **Safe multi-instance deployment**

**After fixing all issues**: ‚úÖ **High-performance multi-instance deployment**

### 9.3 Recommendations

1. **If multi-instance deployment is required**: fix all 11 P0 issues (2‚Äì3 days of work).  
2. **If only demo/single-instance**: current code works.  
3. **Before production**: all P0 and P1 issues must be fixed.

---

## XI. Quick Reference Tables (By Service)

### Gateway Service Features

| Feature | Impact Level | Need Upgrade | Workload |
|--------|--------------|-------------|---------|
| Login / authentication | üü† High | ‚úÖ Yes | distributed lock (0.5 day) |
| Session management | üü† High | ‚úÖ Yes | distributed lock (0.5 day) |
| Token fetch API | üü° Medium | ‚úÖ Yes | store Session in Redis (0.5 day) |

### Game-Service Features

| Feature | Impact Level | Need Upgrade | Workload |
|--------|--------------|-------------|---------|
| Room creation | üü° Medium | ‚úÖ Yes | room index sharding (0.5 day) |
| In-memory Room cache | üü† High | ‚úÖ Yes | cache version / room affinity (1 day) |
| Player move | üü† High | ‚úÖ Yes | CAS failure reload (0.5 day) |
| AI execution | üî¥ Critical | ‚úÖ Yes | distributed lock (0.5 day) |
| Countdown restore | üî¥ Critical | ‚úÖ Yes | distributed lock + SCAN (0.5 day) |
| Redis KEYS (countdown) | üî¥ Critical | ‚úÖ Yes | switch to SCAN (0.5 day) |
| Redis KEYS (seatKey) | üî¥ Critical | ‚úÖ Yes | switch to SCAN (0.5 day) |
| WebSocket broadcast (all operations) | üî¥ Critical | ‚úÖ Yes | external broker or room affinity (1 day) |
| Room join / leave | üî¥ Critical | ‚úÖ Yes | external broker or room affinity (1 day) |
| Seat binding | üü° Medium | ‚úÖ Yes | `deleteSeatKeys` switch to SCAN (0.5 day) |
| Reconnect resume | üü° Medium | ‚ö†Ô∏è Optional | store Session in Redis (0.5 day) |
| Resign / ready / start / restart / kick | üî¥ Critical | ‚úÖ Yes | external broker or room affinity (1 day) |
| Countdown TICK / TIMEOUT | üî¥ Critical | ‚úÖ Yes | external broker or room affinity (1 day) |
| WebSocket connection management | üü° Medium | ‚úÖ Yes | distributed lock (0.5 day) |
| Session invalidated listener | üü° Medium | ‚úÖ Yes | de-dup mechanism (0.5 day) |

### Chat-Service Features

| Feature | Impact Level | Need Upgrade | Workload |
|--------|--------------|-------------|---------|
| Lobby chat | üî¥ Critical | ‚úÖ Yes | external broker (1 day) |
| Room chat | üî¥ Critical | ‚úÖ Yes | external broker (1 day) |
| Private message | üî¥ Critical | ‚úÖ Yes | external broker (1 day) |
| Private session creation | üü° Medium | ‚úÖ Yes | DB unique constraint (0.5 day) |
| Friend relationship check | üü† High | ‚úÖ Yes | security-first (0.5 day) |
| WebSocket connection management | üü° Medium | ‚úÖ Yes | distributed lock (0.5 day) |
| Session invalidated listener | üü° Medium | ‚úÖ Yes | de-dup mechanism (0.5 day) |

### System-Service Features

| Feature | Impact Level | Need Upgrade | Workload |
|--------|--------------|-------------|---------|
| User sync | üü° Medium | ‚úÖ Yes | DB unique constraint (0.5 day) |
| Friend management | üü° Medium | ‚úÖ Yes | DB unique constraint (0.5 day) |
| Session monitoring | üü° Medium | ‚úÖ Yes | SessionRegistry KEYS ‚Üí SCAN (0.5 day) |

### Common Library Features

| Feature | Impact Level | Need Upgrade | Workload |
|--------|--------------|-------------|---------|
| SessionRegistry query (KEYS) | üü° Medium | ‚úÖ Yes | switch to SCAN (0.5 day) |
| Kafka event consumption | üü° Medium | ‚úÖ Yes | de-dup mechanism (0.5 day) |

---

## XII. Refactoring Effort Summary

### Summary by Priority

| Priority | Issue Count | Total Workload | Must Complete By |
|---------|-------------|----------------|------------------|
| **P0 (Critical)** | 11 | 2‚Äì3 days | before multi-instance deployment |
| **P1 (High)** | 4 | 1‚Äì1.5 days | before production |
| **P2 (Medium)** | 9 | 1‚Äì2 days | performance optimization phase |

### Summary by Service

| Service | Features Requiring Refactor | Total Workload |
|--------|-----------------------------|----------------|
| **Gateway** | 3 | 1 day |
| **Game-Service** | 18 | 2‚Äì3 days |
| **Chat-Service** | 8 | 2‚Äì3 days |
| **System-Service** | 3 | 0.5‚Äì1 day |
| **Common libraries** | 2 | 1 day |

### Status After Refactor

**After fixing P0 issues**: ‚úÖ **Can be deployed in multi-instance mode** (all 11 P0 issues must be fixed).  
**After fixing P0 + P1 issues**: ‚úÖ **Safe multi-instance deployment**.  
**After fixing all issues**: ‚úÖ **High-performance multi-instance deployment**.

---

## XIII. Detailed Refactor Plans and Code Examples

### 12.1 Distributed Lock Utility (New)

**File Location**: `game-service/src/main/java/com/gamehub/gameservice/infrastructure/lock/DistributedLockUtil.java`

```java
package com.gamehub.gameservice.infrastructure.lock;

import lombok.RequiredArgsConstructor;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.data.redis.core.script.DefaultRedisScript;
import org.springframework.stereotype.Component;
import java.time.Duration;
import java.util.Collections;

/**
 * Distributed lock utility
 * Used for concurrency control under multi-instance deployment
 */
@Component
@RequiredArgsConstructor
public class DistributedLockUtil {
    private final RedisTemplate<String, Object> redis;
    
    @Value("${instance.id:${spring.application.name}-${random.value}}")
    private String instanceId;
    
    /**
     * Try to acquire a distributed lock
     * @param lockKey lock key
     * @param ttlSeconds TTL in seconds
     * @return true if lock acquired
     */
    public boolean tryLock(String lockKey, int ttlSeconds) {
        Boolean acquired = redis.opsForValue().setIfAbsent(
            lockKey, instanceId, Duration.ofSeconds(ttlSeconds));
        return Boolean.TRUE.equals(acquired);
    }
    
    /**
     * Release lock (only owner can release)
     * Use Lua script for atomic check-and-delete
     */
    public void releaseLock(String lockKey) {
        String script = "if redis.call('get', KEYS[1]) == ARGV[1] then " +
                        "return redis.call('del', KEYS[1]) " +
                        "else return 0 end";
        redis.execute(new DefaultRedisScript<>(script, Long.class), 
                     Collections.singletonList(lockKey), instanceId);
    }
}
```

**Workload**: new file, ~60 LOC.

**Notes**:
- Use Lua script to ensure atomic release and avoid race conditions.
- Lock TTL should be longer than task execution time to avoid lock expiring prematurely.
- For tasks with uncertain duration, consider a ‚Äúlock renewal‚Äù (watchdog) mechanism.

---

### 12.2 AI Duplicate Execution Fix (Detailed Code)

**File Location**: `GomokuWsController.java`

**Before**:
```java
private void maybeScheduleAi(String roomId, GomokuState state, String gameIdAtSchedule) {
    if (state.over() || gomokuService.getMode(roomId) != Mode.PVE) return;
    if (state.current() != gomokuService.getAiPiece(roomId)) return;
    
    // cancel previous AI task for this room
    ScheduledFuture<?> old = pendingAi.remove(roomId);
    if (old != null) old.cancel(false);
    
    // schedule AI task
    ScheduledFuture<?> fut = aiScheduler.schedule(
        () -> {
            runAiTurn(roomId, gameIdAtSchedule, ai);
            pendingAi.remove(roomId);
        }, delay, TimeUnit.MILLISECONDS);
    
    pendingAi.put(roomId, fut);
}
```

**After**:
```java
private final DistributedLockUtil lockUtil;  // new dependency

private void maybeScheduleAi(String roomId, GomokuState state, String gameIdAtSchedule) {
    if (state.over() || gomokuService.getMode(roomId) != Mode.PVE) return;
    if (state.current() != gomokuService.getAiPiece(roomId)) return;
    
    // ‚úÖ new: distributed lock check
    String lockKey = "ai:lock:" + roomId;
    if (!lockUtil.tryLock(lockKey, 5)) {
        log.debug("AI task already running on another instance, skip: roomId={}", roomId);
        return;
    }
    
    try {
        // cancel previous AI task for this room
        ScheduledFuture<?> old = pendingAi.remove(roomId);
        if (old != null) old.cancel(false);
        
        // schedule AI task
        ScheduledFuture<?> fut = aiScheduler.schedule(
            () -> {
                try {
                    runAiTurn(roomId, gameIdAtSchedule, ai);
                } finally {
                    // ‚úÖ new: release lock after execution
                    lockUtil.releaseLock(lockKey);
                    pendingAi.remove(roomId);
                }
            }, delay, TimeUnit.MILLISECONDS);
        
        pendingAi.put(roomId, fut);
    } catch (Exception e) {
        // ‚úÖ new: release lock on exception
        lockUtil.releaseLock(lockKey);
        throw e;
    }
}
```

**Workload**: ~20 LOC changed.

---

### 12.3 Countdown Restore Fix (Detailed Code)

**File Location**: `CountdownSchedulerImpl.java`

**Before**:
```java
@Override
public int restoreAllActive(TimeoutHandler onTimeout) {
    Set<String> keys = redis.keys(stateKey("*"));  // ‚ö†Ô∏è blocking operation
    // ... restore logic ...
}
```

**After**:
```java
@Override
public int restoreAllActive(TimeoutHandler onTimeout) {
    // ‚úÖ new: distributed lock, only one instance performs restore
    String lockKey = "countdown:restore:lock";
    Boolean acquired = redis.opsForValue().setIfAbsent(
        lockKey, nodeId, Duration.ofSeconds(30));
    
    if (!Boolean.TRUE.equals(acquired)) {
        log.info("Another instance is restoring countdowns, skip: nodeId={}", nodeId);
        return 0;
    }
    
    try {
        // ‚úÖ changed: KEYS ‚Üí SCAN
        Set<String> keys = new HashSet<>();
        String cursor = "0";
        String pattern = stateKey("*");
        
        do {
            ScanOptions options = ScanOptions.scanOptions()
                .match(pattern)
                .count(100)  // scan 100 keys each time
                .build();
            Cursor<String> scanCursor = redis.scan(options);
            
            while (scanCursor.hasNext()) {
                keys.add(scanCursor.next());
            }
            cursor = scanCursor.getCursor();
        } while (!"0".equals(cursor));
        
        // ... rest of restore logic ...
        return restored;
    } finally {
        // ‚úÖ new: release lock after restore
        redis.delete(lockKey);
    }
}
```

**Workload**: ~35 LOC (lock + SCAN).

---

### 12.4 SimpleBroker Refactor (Detailed Code)

**File Location**: `WebSocketStompConfig.java`

**Option A: Redis PubSub (Recommended)**

**Before**:
```java
@Override
public void configureMessageBroker(MessageBrokerRegistry registry) {
    registry.enableSimpleBroker("/topic", "/queue");
    registry.setApplicationDestinationPrefixes("/app");
    registry.setUserDestinationPrefix("/user");
}
```

**After**:
```java
@Override
public void configureMessageBroker(MessageBrokerRegistry registry) {
    // ‚úÖ change: SimpleBroker ‚Üí StompBrokerRelay (Redis)
    registry.enableStompBrokerRelay("/topic", "/queue")
        .setRelayHost("${redis.host:localhost}")
        .setRelayPort(${redis.port:6379})
        .setClientLogin("guest")
        .setClientPasscode("guest")
        .setSystemLogin("guest")
        .setSystemPasscode("guest");
    
    registry.setApplicationDestinationPrefixes("/app");
    registry.setUserDestinationPrefix("/user");
}
```

**Option B: Room Affinity (medium change, requires Gateway support)**

Add a routing filter at Gateway:
```java
// gateway/src/main/java/com/gamehub/gateway/filter/RoomAffinityFilter.java
@Component
public class RoomAffinityFilter implements GatewayFilter {
    @Override
    public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) {
        // extract roomId from request
        String roomId = extractRoomId(exchange);
        if (roomId != null) {
            // route to fixed instance based on roomId (consistent hashing)
            String targetInstance = consistentHash(roomId, availableInstances);
            exchange.getRequest().mutate().uri(URI.create(targetInstance));
        }
        return chain.filter(exchange);
    }
}
```

**Workload**:
- Option A: ~10 LOC
- Option B: ~50 LOC (Gateway + config)

---

### 12.5 CAS Failure Reload (Detailed Code)

**File Location**: `GomokuWsController.java`

**Before**:
```java
@MessageMapping("/gomoku.place")
public void place(PlaceCmd cmd, SimpMessageHeaderAccessor sha) {
    // 1. mutate memory state
    GomokuState state = gomokuService.place(roomId, x, y, caller);
    
    // 2. Redis CAS update
    boolean success = gameStateRepository.updateAtomically(...);
    // ‚ö†Ô∏è if fails, memory already dirty, subsequent logic may use it
}
```

**After**:
```java
@MessageMapping("/gomoku.place")
public void place(PlaceCmd cmd, SimpMessageHeaderAccessor sha) {
    try {
        // 1. mutate memory state
        GomokuState state = gomokuService.place(roomId, x, y, caller);
        
        // 2. Redis CAS update
        int expectedStep = computeExpectedStep(state.board()) - 1;
        char expectedTurn = state.current() == 'X' ? 'O' : 'X';
        GameStateRecord rec = buildRecord(state, roomId, gameIdAtSchedule, expectedStep + 1);
        long nextDeadlineMs = state.over() ? 0L : System.currentTimeMillis() + turnSeconds * 1000L;
        
        // ‚úÖ new: reload after CAS failure
        boolean success = gameStateRepository.updateAtomically(
            roomId, gameIdAtSchedule, expectedStep, expectedTurn, rec, nextDeadlineMs);
        
        if (!success) {
            log.warn("CAS failed, reload Redis state to refresh memory: roomId={}", roomId);
            GomokuState freshState = gameStateRepository.load(roomId);
            if (freshState != null) {
                gomokuService.refreshState(roomId, freshState);
            }
            throw new ConcurrentModificationException("Concurrent conflict, please retry");
        }
        
        // ... more logic ...
    } catch (Exception ex) {
        // ... error handling ...
    }
}
```

**Workload**: ~15 LOC changed + new `GomokuService.refreshState()` (~10 LOC).

---

## XIV. Detailed Refactor Timeline

### Phase 1: P0 Core Issues (1‚Äì2 days)

#### Day 1 Morning: AI Duplicate Execution
1. Add `DistributedLockUtil` utility.
2. Modify `GomokuWsController.maybeScheduleAi()`.
3. Single-instance tests (verify lock logic).

#### Day 1 Afternoon: Countdown Duplicate Restore
1. Modify `CountdownSchedulerImpl.restoreAllActive()`.
2. Add distributed lock + SCAN.
3. Single-instance tests (verify SCAN logic).

#### Day 2 Morning: SimpleBroker Refactor
1. ChooseÊñπÊ°à (room affinity recommended for minimal change).
2. Implement routing logic (Gateway or config).
3. Single-instance tests (verify routing).

#### Day 2 Afternoon: Multi-Instance Integration Test
1. Stand up 2‚Äì3 instances.
2. Test AI non-duplication, countdown restore, broadcast correctness.
3. Stress test (concurrent moves, concurrent logins).

### Phase 2: P1 Optimization (0.5‚Äì1 day)

#### Day 3 Morning: CAS Failure Reload
1. Modify `GomokuWsController.place()`.
2. Add `GomokuService.refreshState()`.
3. Test concurrent move scenarios.

#### Day 3 Afternoon: SessionRegistry Optimization
1. Modify `SessionRegistry.registerLoginSessionEnforceSingle()`.
2. Test concurrent login (single-device login).

### Phase 3: P2 Performance Optimization (1‚Äì2 days, optional)

#### Day 4: Other Optimizations
1. Room index sharding.
2. Store Sessions in Redis.
3. DB unique constraints.
4. Kafka de-duplication.

---

## XV. Architecture Before/After Refactor

### Before (Single Instance)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Gateway   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Game-Service (single)     ‚îÇ
‚îÇ   ‚îú‚îÄ pendingAi (memory)     ‚îÇ
‚îÇ   ‚îú‚îÄ SimpleBroker (memory)  ‚îÇ
‚îÇ   ‚îî‚îÄ restoreAllActive (no lock) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Redis    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### After (Multi-Instance)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Gateway   ‚îÇ
‚îÇ  (routing)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚ñº                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Game-Service (1) ‚îÇ  ‚îÇ Game-Service (2) ‚îÇ
‚îÇ ‚îú‚îÄ DistributedLock‚îÇ ‚îÇ ‚îú‚îÄ DistributedLock‚îÇ
‚îÇ ‚îú‚îÄ Redis PubSub  ‚îÇ  ‚îÇ ‚îú‚îÄ Redis PubSub  ‚îÇ
‚îÇ ‚îî‚îÄ SCAN restore  ‚îÇ  ‚îÇ ‚îî‚îÄ SCAN restore  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                      ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ    Redis    ‚îÇ
            ‚îÇ (shared)    ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## XVI. Risk Assessment and Rollback Plan

### Refactor Risks

| Risk | Level | Mitigation |
|------|-------|-----------|
| **Distributed lock deadlock** | Low | TTL auto-release + `finally` release |
| **Lock contention performance** | Low | Fine-grained locks (per roomId), low contention probability |
| **SCAN missing data** | Low | Restore is acceptable as eventually consistent |
| **External broker failure** | Medium | Room affinityÊñπÊ°à does not rely on external broker |

### Rollback Plan

If refactor causes issues:

1. **Quick rollback**: remove distributed lock logic and revert to single-instance deployment.  
2. **Gradual rollback**: fix AI and countdown only, keep SimpleBroker with room affinity.  
3. **Partial rollback**: rollback only problematic features while keeping other fixes.

---

**Analysis completion time**: 2025-12-17
