# 1.0 版本多实例部署影响分析

> **文档版本**：1.0  
> **分析日期**：2025-12-17  
> **适用范围**：当前 1.0 版本的所有业务代码、所有服务、所有功能点  
> **目标**：明确当前版本在多实例部署下的问题，并给出对应的升级方案

---

## 一、文档说明

### 1.1 分析范围

本文档针对 **1.0 版本**的所有功能点，逐一分析多实例部署的影响：

- **Gateway 服务**：登录/鉴权、会话管理、Token 管理
- **Game-Service 服务**：房间管理、对局状态、AI 执行、倒计时、WebSocket 广播
- **Chat-Service 服务**：大厅聊天、房间聊天、私聊、WebSocket 广播
- **System-Service 服务**：用户管理、好友关系、会话监控
- **公共库**：SessionRegistry、Kafka 事件

**说明**：本文档仅包含**有影响**的功能点，无影响的功能点已排除。

### 1.2 分析维度

对每个功能点，分析以下维度：

1. **功能描述**：当前实现的功能
2. **多实例影响**：多实例部署会产生什么影响
3. **影响原因**：为什么会产生这个影响（技术原因）
4. **影响等级**：🔴 致命 / 🟠 严重 / 🟡 中等 / 🟢 轻微
5. **升级方案**：对应的升级方案（不改动架构的前提下）

---

## 二、总体问题概览

从当前代码实现来看，多实例部署下的核心问题可以粗略归纳为四大类，其余具体条目基本都可以挂在这四类之下理解：

1. **房间亲和问题（Room Affinity）**  
   - 同一房间的 HTTP / WebSocket 请求在负载均衡下可能被路由到**不同实例**，而业务代码默认假设“同一房间长期落在同一个实例”。  
   - 典型影响：AI 延迟落子调度可以在多个实例上各自触发一次、倒计时恢复在多个实例上各自扫描/恢复、SimpleBroker 广播只发到本实例连接，看不到其他实例的玩家等。  
   - 改造路径：网关按 `roomId` 做房间亲和路由；WebSocket 广播从内存 `SimpleBroker` 换成外部 broker（如 RabbitMQ StompRelay / Redis PubSub）以实现跨实例广播。

2. **多实例变量不共享（In-Memory State Not Shared）**  
   - 关键状态如 `rooms`、`pendingAi`、本地缓存、定时任务持有的状态等都存放在**各实例自己的 JVM 内存**中，而不是集中在 Redis 或通过事件同步。  
   - 在多实例场景下，这些变量彼此**不共享、不一致**，会导致：同一房间在 A/B 两个实例的内存中出现不同棋盘状态、同时调度两次 AI、倒计时任务重复或缺失等。

3. **幂等与重试问题（Idempotency & Retry）**  
   - 当前只在少数关键点做了幂等（如落子 CAS、私聊 `clientOpId`），整体上 **HTTP / WebSocket / Kafka 的幂等设计不统一**。  
   - 在网络抖动、重试、断线重连、多实例并发等场景下，可能出现：重复创建房间/好友申请、WS 指令重复执行导致状态抖动、Kafka 事件被重复消费后未来的非幂等逻辑（记战绩/发奖）重复执行等。

4. **WebSocket 广播单实例隔离（SimpleBroker Limitations）**  
   - 当前 game-service 与 chat-service 都使用 `enableSimpleBroker("/topic", "/queue")`，`SimpleBroker` 只在**单个实例内部**管理订阅与广播。  
   - 结果是：某条 `/topic/...` 消息只会推给“连接在本实例上的订阅者”，**跨实例的订阅者完全收不到**：  
     - 大厅 `/topic/chat.lobby` 在多实例下实际上变成“每个实例一个小大厅”；  
     - 房间 `/topic/room.{roomId}` 如果房间成员连接在不同实例，彼此也收不到对方的消息。  
   - 需要在后续版本中，将 STOMP 广播从内存 `SimpleBroker` 升级为外部消息代理（如 RabbitMQ StompRelay / Redis PubSub），实现真正的跨实例广播。

以下各章节的“多实例影响”，都可以映射回上述三类之一或几类的组合：  
要么是**路由不亲和**导致“同一房间落到多个实例”，要么是**关键状态仅在内存，不在 Redis/事件里统一**，要么是在**重试/并发场景下缺少统一幂等防护**。

---

## 三、Gateway 服务多实例影响分析

### 2.1 登录/鉴权功能

#### 功能描述
- **位置**：`LoginSessionKickHandler.onAuthenticationSuccess()`
- **功能**：用户登录成功后，注册会话到 SessionRegistry，踢掉旧会话，写黑名单，发 Kafka 事件

#### 多实例影响
- **影响**：两个实例同时处理同一用户的登录请求时，可能都注册成功，导致单设备登录失效
- **影响等级**：🟠 **严重**

#### 影响原因
- `SessionRegistry.registerLoginSessionEnforceSingle()` 不是完全原子操作
- 两个实例同时查询 ACTIVE 会话（可能都返回空），然后都注册新会话
- 虽然有 SETNX，但查询和注册之间存在时间窗口

#### 升级方案
```java
// 在 registerLoginSessionEnforceSingle() 前加分布式锁
String lockKey = "session:register:" + userId;
Boolean acquired = redis.opsForValue().setIfAbsent(lockKey, instanceId, Duration.ofSeconds(5));
if (!Boolean.TRUE.equals(acquired)) {
    // 等待 100ms 后重试
    Thread.sleep(100);
    return registerLoginSessionEnforceSingle(...); // 递归重试
}
try {
    // 执行注册逻辑
} finally {
    redis.delete(lockKey);
}
```

---

### 2.2 会话管理（SessionRegistry）

#### 功能描述
- **位置**：`SessionRegistry.registerLoginSessionEnforceSingle()`
- **功能**：注册登录会话，标记旧会话为 KICKED，返回被踢的会话列表

#### 多实例影响
- **影响**：并发注册时，两个实例可能都认为"没有 ACTIVE 会话"，都注册成功
- **影响等级**：🟠 **严重**

#### 影响原因
- 查询 ACTIVE 会话 → 标记旧会话 → 注册新会话，不是原子操作
- 两个实例几乎同时执行时，查询阶段可能都看不到对方的会话

#### 升级方案
- 同 2.1，加分布式锁

---

### 2.3 Token 获取接口

#### 功能描述
- **位置**：`TokenController.getToken()`
- **功能**：从 HTTP Session 获取 access_token，返回给前端

#### 多实例影响
- **影响**：如果用户登录在实例 1，但后续请求路由到实例 2，可能获取不到 token（如果 Session 不共享）
- **影响等级**：🟡 **中等**

#### 影响原因
- Spring WebFlux 的 Session 默认存储在内存，不跨实例共享
- 如果网关层没有 Session 亲和，用户可能连到不同实例

#### 升级方案
- **方案1**：Session 存储到 Redis（推荐）
  ```yaml
  spring:
    session:
      store-type: redis
  ```
- **方案2**：网关层做 Session 亲和（按 userId 路由到固定实例）

---

## 三、Game-Service 服务多实例影响分析

### 3.1 房间创建

#### 功能描述
- **位置**：`GomokuRestController.newRoom()`
- **功能**：创建新房间，保存房间元信息到 Redis，添加到房间索引（ZSET）

#### 多实例影响
- **影响**：房间索引写冲突（单 ZSET key），但不会导致数据错误，只是性能下降
- **影响等级**：🟡 **中等**

#### 影响原因
- 所有房间索引都在同一个 ZSET key（`gomoku:rooms:index`）
- 多实例并发创建房间时，写同一个 key，Redis 内部会串行化，但延迟增加

#### 升级方案
```java
// 分桶：按 roomId 哈希分到多个桶
public static String roomIndexKey(String roomId) {
    int bucket = Math.abs(roomId.hashCode()) % 100;
    return "gomoku:rooms:index:" + bucket;
}
```

---

### 3.2 内存 Room 缓存（多实例不一致）

#### 功能描述
- **位置**：`GomokuServiceImpl.rooms`
- **功能**：内存缓存 Room 对象，避免频繁从 Redis 加载

#### 多实例影响
- **影响**：多实例下，每个实例都有自己的 `rooms` Map，同一房间的 Room 对象可能不一致
- **影响等级**：🟠 **严重**

#### 影响原因
- `GomokuServiceImpl.rooms` 是内存 `ConcurrentHashMap`，每个实例独立
- `room()` 方法：先查内存，没有则从 Redis 加载并放入内存
- 多实例下，实例 1 和实例 2 各自缓存了同一房间的 Room 对象
- 如果实例 1 修改了 Room（如绑定座位），实例 2 的内存 Room 对象不会自动更新
- Room 对象内部还有多个内存 Map（`seatBySession`, `seatKeyToSeat`, `seatToSessionId`），这些状态在多实例下可能不一致

#### 升级方案
```java
// 方案1：每次操作前都从 Redis 刷新（性能较差，但最安全）
private Room room(String roomId) {
    // 每次都从 Redis 加载，不缓存
    RoomMeta meta = roomRepo.getRoomMeta(roomId)...
    Room r = RoomMetaConverter.toRoom(meta);
    // ... 回灌座位绑定和游戏状态 ...
    return r;
}

// 方案2：缓存 + 版本号（推荐）
// 在 RoomMeta 中增加 version 字段，每次更新 version++
// 操作前检查 version，如果版本不一致则刷新缓存
private Room room(String roomId) {
    Room cached = rooms.get(roomId);
    if (cached != null) {
        RoomMeta meta = roomRepo.getRoomMeta(roomId).orElse(null);
        if (meta != null && meta.getVersion() == cached.getVersion()) {
            return cached; // 版本一致，使用缓存
        }
        // 版本不一致，刷新缓存
        rooms.remove(roomId);
    }
    // 从 Redis 加载并缓存
    // ...
}

// 方案3：房间亲和（最简单，但需要网关配合）
// 同一房间的所有操作都路由到同一实例，避免多实例并发
```

---

### 3.3 玩家落子（并发控制）

#### 功能描述
- **位置**：`GomokuWsController.place()`
- **功能**：处理玩家落子，更新游戏状态，CAS 更新 Redis

#### 多实例影响
- **影响**：CAS 失败率增加，但不会导致数据错误；CAS 失败后内存状态可能脏；如果使用内存 Room 缓存，可能基于脏状态判断
- **影响等级**：🟠 **严重**（CAS 失败后脏数据 + 内存 Room 缓存不一致）

#### 影响原因
- 流程：内存 Room 先变更 → Redis CAS 更新
- 如果 CAS 失败，内存状态已变但 Redis 未更新，后续逻辑（如 AI 判断）可能用到脏状态
- 如果使用内存 Room 缓存，多实例下可能基于不同版本的状态判断

#### 升级方案
```java
// CAS 失败后立即回读 Redis 刷新内存
boolean success = gameStateRepository.updateAtomically(...);
if (!success) {
    // 清除内存 Room 缓存，强制下次从 Redis 加载
    rooms.remove(roomId);
    GomokuState freshState = gameStateRepository.load(roomId);
    gomokuService.refreshState(roomId, freshState);
    throw new ConcurrentModificationException("并发冲突，请重试");
}
```

---

### 3.4 AI 执行（PVE 模式）

#### 功能描述
- **位置**：`GomokuWsController.maybeScheduleAi()`、`runAiTurn()`
- **功能**：PVE 模式下，轮到 AI 时延迟执行 AI 落子

#### 多实例影响
- **影响**：同一房间的 AI 可能在多个实例上同时执行，导致 AI 落子重复、游戏状态混乱
- **影响等级**：🔴 **致命**

#### 影响原因
- `pendingAi` 是内存变量（`ConcurrentHashMap`），每个实例独立
- 多实例部署时，同一房间的玩家可能连接到不同实例
- 两个实例都判断"轮到 AI"，各自调度 AI 任务

#### 升级方案
```java
// 在 maybeScheduleAi() 前加分布式锁
String lockKey = "ai:lock:" + roomId;
Boolean acquired = redis.opsForValue().setIfAbsent(lockKey, instanceId, Duration.ofSeconds(5));
if (!Boolean.TRUE.equals(acquired)) {
    log.debug("AI 任务已在其他实例执行，跳过: roomId={}", roomId);
    return;
}
try {
    // 执行 AI 逻辑
} finally {
    redis.delete(lockKey);
}
```

---

### 3.5 倒计时恢复（服务启动）

#### 功能描述
- **位置**：`CountdownSchedulerImpl.restoreAllActive()`
- **功能**：服务启动时恢复所有未过期的倒计时任务

#### 多实例影响
- **影响**：多实例同时启动时，每个实例都会恢复所有倒计时，导致倒计时"加速"（30 秒变成 10 秒）
- **影响等级**：🔴 **致命**

#### 影响原因
- 每个实例在 `ApplicationReadyEvent` 时执行 `restoreAllActive()`
- 每个实例都扫描 `countdown:*` 并恢复所有倒计时
- 同一倒计时被多个实例同时调度

#### 升级方案
```java
// 在 restoreAllActive() 前加分布式锁
String lockKey = "countdown:restore:lock";
Boolean acquired = redis.opsForValue().setIfAbsent(lockKey, nodeId, Duration.ofSeconds(30));
if (!Boolean.TRUE.equals(acquired)) {
    log.info("其他实例正在恢复倒计时，跳过");
    return 0;
}
try {
    // 执行恢复逻辑
} finally {
    redis.delete(lockKey);
}
```

---

### 3.6 倒计时恢复（Redis KEYS 阻塞）

#### 功能描述
- **位置**：`CountdownSchedulerImpl.restoreAllActive()` 
- **功能**：使用 `redis.keys("countdown:*")` 扫描所有倒计时 key

#### 多实例影响
- **影响**：多实例同时启动时，多个实例同时执行 KEYS 命令，Redis 主线程被阻塞，所有请求延迟飙升
- **影响等级**：🔴 **致命**

#### 影响原因
- `KEYS` 是阻塞全库扫描，key 多的时候会阻塞 Redis 主线程
- 多实例同时执行 KEYS，Redis 被多个阻塞命令占满

#### 升级方案
```java
// 改用 SCAN 游标遍历
Set<String> keys = new HashSet<>();
String cursor = "0";
do {
    ScanOptions options = ScanOptions.scanOptions()
        .match(stateKey("*"))
        .count(100)
        .build();
    Cursor<String> scanCursor = redis.scan(options);
    while (scanCursor.hasNext()) {
        keys.add(scanCursor.next());
    }
    cursor = scanCursor.getCursor();
} while (!"0".equals(cursor));
```

---

### 3.7 WebSocket 广播（SimpleBroker）

#### 功能描述
- **位置**：`WebSocketStompConfig.configureMessageBroker()`
- **功能**：使用 SimpleBroker 做内存消息代理，广播到 `/topic/room.{roomId}`

#### 多实例影响
- **影响**：如果房间的玩家连接到不同实例，广播会失败（玩家看不到对手的落子）
- **影响等级**：🔴 **致命**

#### 影响原因
- SimpleBroker 是内存消息代理，只能在单实例内广播
- 如果玩家 1 连接到实例 1，玩家 2 连接到实例 2，实例 1 的广播只有实例 1 的订阅者能收到

#### 升级方案
- **方案1**：房间亲和（改动小）
  ```java
  // 在网关层按 roomId 做一致性哈希路由
  String targetInstance = consistentHash(roomId, availableInstances);
  ```
- **方案2**：外部 broker（Redis PubSub 或 RabbitMQ）
  ```java
  registry.enableStompBrokerRelay("/topic", "/queue")
      .setRelayHost("redis-host")
      .setRelayPort(6379);
  ```

---

### 3.8 房间加入

#### 功能描述
- **位置**：`GomokuRestController.joinRoom()`
- **功能**：玩家加入房间，自动分配座位，广播 SNAPSHOT

#### 多实例影响
- **影响**：如果房间的玩家连接到不同实例，SNAPSHOT 广播可能失败（SimpleBroker 限制）
- **影响等级**：🔴 **致命**（广播失败）

#### 影响原因
- `broadcastSnapshot()` 使用 SimpleBroker，只能在单实例内广播
- 如果房间内玩家连接到不同实例，部分玩家收不到 SNAPSHOT

#### 升级方案
- 同 3.7，改用外部 broker 或房间亲和

---

### 3.9 房间退出

#### 功能描述
- **位置**：`GomokuRestController.leaveRoom()`
- **功能**：玩家退出房间，清理座位绑定，广播 SNAPSHOT

#### 多实例影响
- **影响**：如果房间的玩家连接到不同实例，SNAPSHOT 广播可能失败（SimpleBroker 限制）
- **影响等级**：🔴 **致命**（广播失败）

#### 影响原因
- `broadcastSnapshot()` 使用 SimpleBroker，只能在单实例内广播
- 如果房间内玩家连接到不同实例，部分玩家收不到 SNAPSHOT

#### 升级方案
- 同 3.7，改用外部 broker 或房间亲和

---

### 3.10 座位绑定（seatKey）

#### 功能描述
- **位置**：`RedisRoomRepository.setSeatKey()`、`GomokuResumeController.onResume()`
- **功能**：通过 seatKey 绑定座位（用于重连恢复）

#### 多实例影响
- **影响**：`setSeatKey()` 无影响（Redis SETNX 是原子的）；但 `deleteSeatKeys()` 使用 KEYS，会阻塞
- **影响等级**：🟡 **中等**（deleteSeatKeys 阻塞）

#### 影响原因
- `setSeatKey()` 使用 Redis SETNX，是原子操作，多实例并发不会冲突
- `deleteSeatKeys()` 使用 `redis.keys()`，是阻塞操作，多实例同时执行会阻塞 Redis

#### 升级方案
```java
// deleteSeatKeys() 改用 SCAN
Set<String> keys = new HashSet<>();
String cursor = "0";
String pattern = RedisKeys.roomSeatKeyPrefix(roomId) + "*";
do {
    ScanOptions options = ScanOptions.scanOptions()
        .match(pattern)
        .count(100)
        .build();
    Cursor<String> scanCursor = redis.scan(options);
    while (scanCursor.hasNext()) {
        keys.add(scanCursor.next());
    }
    cursor = scanCursor.getCursor();
} while (!"0".equals(cursor));
if (!keys.isEmpty()) {
    redisTemplate.delete(keys);
}
```

---

### 3.11 重连恢复

#### 功能描述
- **位置**：`GomokuResumeController.onResume()`
- **功能**：客户端重连时，通过 seatKey 恢复座位，返回完整快照

#### 多实例影响
- **影响**：如果重连到不同实例，可能找不到之前的 WebSocket Session（如果 Session 不共享）
- **影响等级**：🟡 **中等**

#### 影响原因
- WebSocket Session 默认存储在内存，不跨实例共享
- 如果用户重连到不同实例，之前的 Session 信息丢失（但 seatKey 在 Redis，可以恢复）

#### 升级方案
- 如果 Session 信息需要跨实例共享，使用 Redis Session；否则无需改造（seatKey 已足够）

---

### 3.12 认输功能

#### 功能描述
- **位置**：`GomokuWsController.resign()`
- **功能**：玩家认输，更新游戏状态，广播结果

#### 多实例影响
- **影响**：同 3.3，CAS 失败后可能脏数据；广播可能失败
- **影响等级**：🟠 **严重**（CAS 失败）+ 🔴 **致命**（广播失败）

#### 影响原因
- 同 3.3（CAS 失败后脏数据）
- 同 3.7（SimpleBroker 广播失败）

#### 升级方案
- 同 3.3（CAS 失败回读）+ 同 3.7（外部 broker）

---

### 3.13 准备/开始/重开功能

#### 功能描述
- **位置**：`GomokuWsController.ready()`、`start()`、`restart()`
- **功能**：玩家准备、开始游戏、重开游戏，更新房间状态，广播 SNAPSHOT

#### 多实例影响
- **影响**：广播可能失败（SimpleBroker 限制）
- **影响等级**：🔴 **致命**（广播失败）

#### 影响原因
- 同 3.7

#### 升级方案
- 同 3.7

---

### 3.14 踢人功能

#### 功能描述
- **位置**：`GomokuWsController.kick()`
- **功能**：房主踢出其他玩家，清理座位，广播 SNAPSHOT

#### 多实例影响
- **影响**：广播可能失败（SimpleBroker 限制）
- **影响等级**：🔴 **致命**（广播失败）

#### 影响原因
- 同 3.7

#### 升级方案
- 同 3.7

---

### 3.15 倒计时 TICK 事件

#### 功能描述
- **位置**：`TurnClockCoordinator.onReady()`、`CountdownScheduler.tickListener`
- **功能**：每秒触发 TICK 事件，广播倒计时剩余时间

#### 多实例影响
- **影响**：如果房间的玩家连接到不同实例，可能收不到 TICK 事件（SimpleBroker 限制）
- **影响等级**：🔴 **致命**（广播失败）

#### 影响原因
- TICK 事件通过 SimpleBroker 广播到 `/topic/room.{roomId}`
- 同 3.7，SimpleBroker 无法跨实例

#### 升级方案
- 同 3.7

---

### 3.16 倒计时 TIMEOUT 事件

#### 功能描述
- **位置**：`TurnClockCoordinator.handleTimeout()`
- **功能**：倒计时超时，判负，广播 TIMEOUT 事件

#### 多实例影响
- **影响**：广播可能失败（SimpleBroker 限制）；但超时处理有 holder 锁保护，不会重复执行
- **影响等级**：🔴 **致命**（广播失败）

#### 影响原因
- 超时处理有 holder 锁（`tryAcquireHolder()`），不会重复执行
- 但广播使用 SimpleBroker，可能失败

#### 升级方案
- 同 3.7

---

### 3.17 WebSocket 连接管理（连接建立）

#### 功能描述
- **位置**：`WebSocketSessionManager.handleSessionConnect()`
- **功能**：WebSocket 连接建立时，注册会话到 SessionRegistry，踢掉同服务的旧连接

#### 多实例影响
- **影响**：如果用户同时在两个实例上建立连接，可能都注册成功（如果查询和注册之间存在时间窗口）
- **影响等级**：🟡 **中等**

#### 影响原因
- 查询旧连接 → 删除旧连接 → 注册新连接，不是原子操作
- 两个实例几乎同时处理连接时，可能都认为"没有旧连接"，都注册成功

#### 升级方案
```java
// 在注册前加分布式锁
String lockKey = "ws:register:" + userId + ":game-service";
Boolean acquired = redis.opsForValue().setIfAbsent(lockKey, instanceId, Duration.ofSeconds(5));
if (!Boolean.TRUE.equals(acquired)) {
    // 其他实例正在注册，等待后重试
    Thread.sleep(100);
    // 重新查询并注册
}
try {
    // 执行注册逻辑
} finally {
    redis.delete(lockKey);
}
```

---

### 3.18 Session 失效监听（Kafka 消费）

#### 功能描述
- **位置**：`SessionInvalidatedListener.onSessionInvalidated()`
- **功能**：消费 Kafka 会话失效事件，断开用户的 WebSocket 连接

#### 多实例影响
- **影响**：如果消息重复消费，可能重复断开（但断开是幂等的）
- **影响等级**：🟡 **中等**

#### 影响原因
- Kafka 可能重复投递消息
- 当前没有去重机制，重复消费会重复调用断开逻辑

#### 升级方案
- 同 6.2，加去重机制

---

## 四、Chat-Service 服务多实例影响分析

### 4.1 大厅聊天

#### 功能描述
- **位置**：`ChatWsController.sendLobby()`、`ChatMessagingService.sendLobbyMessage()`
- **功能**：发送大厅消息，广播到 `/topic/chat.lobby`

#### 多实例影响
- **影响**：如果用户连接到不同实例，可能收不到消息（SimpleBroker 限制）
- **影响等级**：🔴 **致命**

#### 影响原因
- 使用 SimpleBroker，只能在单实例内广播
- 如果用户 A 连接到实例 1，用户 B 连接到实例 2，用户 A 发的消息只有实例 1 的订阅者能收到

#### 升级方案
- 改用外部 broker（Redis PubSub 或 RabbitMQ），必须用外部 broker（大厅聊天不适合房间亲和）

---

### 4.2 房间聊天

#### 功能描述
- **位置**：`ChatWsController.sendRoom()`、`ChatMessagingService.sendRoomMessage()`
- **功能**：发送房间消息，广播到 `/topic/chat.room.{roomId}`，并保存到 Redis List

#### 多实例影响
- **影响**：如果房间的玩家连接到不同实例，可能收不到消息（SimpleBroker 限制）
- **影响等级**：🔴 **致命**

#### 影响原因
- 同 4.1，SimpleBroker 限制

#### 升级方案
- 改用外部 broker 或房间亲和

---

### 4.3 私聊消息

#### 功能描述
- **位置**：`ChatWsController.sendPrivate()`、`ChatMessagingService.sendPrivateMessage()`
- **功能**：发送私聊消息，使用点对点推送（`/user/queue/chat.private`），保存到 PostgreSQL

#### 多实例影响
- **影响**：如果目标用户连接到不同实例，可能收不到消息（SimpleBroker 限制）
- **影响等级**：🔴 **致命**

#### 影响原因
- `convertAndSendToUser()` 依赖 SimpleBroker，只能在单实例内路由
- 如果目标用户在实例 2，但消息从实例 1 发送，实例 1 的 SimpleBroker 找不到实例 2 的用户

#### 升级方案
- 改用外部 broker（Redis PubSub 或 RabbitMQ），支持跨实例用户路由

---

### 4.4 私聊会话创建

#### 功能描述
- **位置**：`ChatSessionService.getOrCreatePrivateSession()`
- **功能**：获取或创建私聊会话（数据库操作）

#### 多实例影响
- **影响**：可能创建重复会话（如果两个实例同时为同一对用户创建会话）
- **影响等级**：🟡 **中等**

#### 影响原因
- 查询会话 → 不存在则创建，不是原子操作
- 两个实例几乎同时执行时，可能都认为"不存在"，都创建会话

#### 升级方案
```java
// 使用数据库唯一约束（userId1, userId2）防止重复
// 或者在应用层加分布式锁
String lockKey = "chat:session:create:" + min(userId1, userId2) + ":" + max(userId1, userId2);
```

---

### 4.5 好友关系校验

#### 功能描述
- **位置**：`ChatMessagingServiceImpl.sendPrivateMessage()` 第 107 行
- **功能**：发送私聊前，通过 Feign 调用 system-service 验证好友关系

#### 多实例影响
- **影响**：如果 Feign 调用失败（401），当前逻辑是"降级允许发送"，存在安全风险
- **影响等级**：🟠 **严重**（安全风险）

#### 影响原因
- 当前代码：`catch (FeignException.Unauthorized e)` 时，记录警告但继续发送
- 多实例下，如果 system-service 不可用，所有实例都会降级，安全风险放大

#### 升级方案
```java
// 改为安全优先，验证失败则拒绝发送
catch (FeignException.Unauthorized e) {
    log.error("验证好友关系时 Token 无效: senderId={}, targetUserId={}", senderId, targetUserId);
    return false; // 拒绝发送
}
```

---

### 4.6 WebSocket 连接管理（连接建立）

#### 功能描述
- **位置**：`WebSocketSessionManager.handleSessionConnect()`
- **功能**：WebSocket 连接建立时，注册会话到 SessionRegistry，踢掉同服务的旧连接

#### 多实例影响
- **影响**：如果用户同时在两个实例上建立连接，可能都注册成功（如果查询和注册之间存在时间窗口）
- **影响等级**：🟡 **中等**

#### 影响原因
- 查询旧连接 → 删除旧连接 → 注册新连接，不是原子操作
- 两个实例几乎同时处理连接时，可能都认为"没有旧连接"，都注册成功

#### 升级方案
- 同 3.17，加分布式锁

---

### 4.7 Session 失效监听（Kafka 消费）

#### 功能描述
- **位置**：`SessionInvalidatedListener.onSessionInvalidated()`
- **功能**：消费 Kafka 会话失效事件，断开用户的 WebSocket 连接

#### 多实例影响
- **影响**：如果消息重复消费，可能重复断开（但断开是幂等的）
- **影响等级**：🟡 **中等**

#### 影响原因
- Kafka 可能重复投递消息
- 当前没有去重机制，重复消费会重复调用断开逻辑

#### 升级方案
- 同 6.2，加去重机制

---

### 4.8 WebSocket Token 存储（降级场景）

#### 功能描述
- **位置**：`WebSocketTokenStore.fallbackStore`
- **功能**：WebSocket Token 存储，优先使用 Redis，Redis 不可用时降级到内存存储

#### 多实例影响
- **影响**：正常情况下使用 Redis，无影响；但如果 Redis 不可用，降级到内存存储（`fallbackStore`），多实例下无法共享
- **影响等级**：🟡 **中等**（仅在 Redis 不可用时）

#### 影响原因
- `fallbackStore` 是内存 `ConcurrentHashMap`，每个实例独立
- 如果 Redis 不可用，实例 1 存储的 token，实例 2 无法访问

#### 升级方案
- **方案1**：确保 Redis 高可用，避免降级到内存存储（推荐）
- **方案2**：如果必须支持降级，在降级模式下限制为单实例部署，或使用其他共享存储（如数据库）

---

## 五、System-Service 服务多实例影响分析

### 5.1 用户同步

#### 功能描述
- **位置**：`UserController.syncUser()`
- **功能**：从 Keycloak 同步用户信息到系统数据库

#### 多实例影响
- **影响**：可能重复同步（如果两个实例同时处理同一用户的同步请求）
- **影响等级**：🟡 **中等**

#### 影响原因
- 查询用户 → 不存在则创建，不是原子操作
- 两个实例几乎同时执行时，可能都认为"不存在"，都创建用户

#### 升级方案
```java
// 使用数据库唯一约束（keycloak_user_id）防止重复
// 或者在应用层加分布式锁
String lockKey = "user:sync:" + keycloakUserId;
```

---

### 5.2 好友关系管理

#### 功能描述
- **位置**：`FriendController.applyFriend()`、`accept()`、`reject()`
- **功能**：申请加好友、同意/拒绝好友申请（数据库操作）

#### 多实例影响
- **影响**：如果数据库没有唯一约束，可能重复申请
- **影响等级**：🟡 **中等**

#### 影响原因
- 需要查看数据库表结构，如果没有唯一约束（如 `(requester_id, receiver_id)`），两个实例同时处理可能重复申请

#### 升级方案
```java
// 使用数据库唯一约束（requester_id, receiver_id）防止重复
// 或者在应用层加分布式锁
String lockKey = "friend:apply:" + min(userId1, userId2) + ":" + max(userId1, userId2);
```

---

### 5.3 会话监控

#### 功能描述
- **位置**：`SessionMonitorController.getAllSessions()`
- **功能**：查询所有在线会话（从 SessionRegistry）

#### 多实例影响
- **影响**：如果 SessionRegistry 使用 `redis.keys()` 查询，多实例同时查询会阻塞 Redis
- **影响等级**：🟡 **中等**

#### 影响原因
- 需要查看 `SessionRegistry.getUserIds()` 的实现，如果使用 KEYS，会有阻塞风险

#### 升级方案
- 如果使用 KEYS，改为 SCAN

---

## 六、公共库多实例影响分析

### 6.1 SessionRegistry 查询（KEYS 阻塞）

#### 功能描述
- **位置**：`SessionRegistry.getUserIds()`（第 591 行）
- **功能**：获取所有拥有会话的用户 ID（用于 `getAllUserSessions()`、`getAllUsersWithSessions()`）

#### 多实例影响
- **影响**：使用 `redis.keys()` 查询，多实例同时查询会阻塞 Redis
- **影响等级**：🟡 **中等**

#### 影响原因
- `getUserIds()` 使用 `redis.keys(prefix + "*")`，是阻塞操作
- 如果在线用户多（比如 10 万），KEYS 会阻塞 Redis 几秒
- 多实例同时调用（如系统监控接口），会放大阻塞影响

#### 升级方案
```java
// 改用 SCAN
private Set<String> getUserIds(String prefix) {
    Set<String> userIds = new HashSet<>();
    String cursor = "0";
    do {
        ScanOptions options = ScanOptions.scanOptions()
            .match(prefix + "*")
            .count(100)
            .build();
        Cursor<String> scanCursor = redis.scan(options);
        while (scanCursor.hasNext()) {
            String key = scanCursor.next();
            userIds.add(key.substring(prefix.length()));
        }
        cursor = scanCursor.getCursor();
    } while (!"0".equals(cursor));
    return userIds;
}
```

---

### 6.2 Kafka 事件消费

#### 功能描述
- **位置**：`SessionEventConsumer.onMessage()`、`SessionInvalidatedListener.onSessionInvalidated()`
- **功能**：消费会话失效事件，断开 WebSocket 连接

#### 多实例影响
- **影响**：如果消息重复消费，可能重复断开（但断开是幂等的）
- **影响等级**：🟡 **中等**

#### 影响原因
- Kafka 可能重复投递消息
- 当前没有去重机制，重复消费会重复调用断开逻辑

#### 升级方案
```java
// 加去重：用 Redis SET 记录已处理的 eventId
String eventId = userId + ":" + loginSessionId + ":" + timestamp;
Boolean processed = redis.opsForValue().setIfAbsent("event:processed:" + eventId, "1", Duration.ofMinutes(5));
if (!Boolean.TRUE.equals(processed)) {
    log.debug("事件已处理，跳过: eventId={}", eventId);
    return;
}
// 执行断开逻辑
```

---

## 七、影响汇总与优先级

### 7.1 致命问题（🔴 P0，必须修复）

| 功能点 | 服务 | 影响 | 升级方案 |
|--------|------|------|---------|
| AI 执行 | game-service | 多实例重复执行 | 分布式锁 |
| 倒计时恢复 | game-service | 多实例重复恢复 | 分布式锁 + SCAN |
| Redis KEYS（倒计时） | game-service | 多实例阻塞 Redis | 改用 SCAN |
| Redis KEYS（seatKey） | game-service | 多实例阻塞 Redis | 改用 SCAN |
| Redis KEYS（SessionRegistry） | 公共库 | 多实例阻塞 Redis | 改用 SCAN |
| WebSocket 广播（对局） | game-service | SimpleBroker 无法跨实例 | 外部 broker 或房间亲和 |
| WebSocket 广播（房间操作） | game-service | SimpleBroker 无法跨实例 | 外部 broker 或房间亲和 |
| WebSocket 广播（倒计时 TICK） | game-service | SimpleBroker 无法跨实例 | 外部 broker 或房间亲和 |
| 大厅聊天广播 | chat-service | SimpleBroker 无法跨实例 | 外部 broker |
| 房间聊天广播 | chat-service | SimpleBroker 无法跨实例 | 外部 broker |
| 私聊消息推送 | chat-service | SimpleBroker 无法跨实例 | 外部 broker |

### 7.2 严重问题（🟠 P1，上线前修复）

| 功能点 | 服务 | 影响 | 升级方案 |
|--------|------|------|---------|
| 登录会话注册 | gateway | 并发注册导致单设备登录失效 | 分布式锁 |
| 内存 Room 缓存 | game-service | 多实例缓存不一致 | 缓存版本号或房间亲和 |
| CAS 失败后脏数据 | game-service | 内存状态不一致 | CAS 失败后回读 |
| 好友关系校验降级 | chat-service | 安全风险 | 改为安全优先 |

### 7.3 中等问题（🟡 P2，性能优化）

| 功能点 | 服务 | 影响 | 升级方案 |
|--------|------|------|---------|
| 房间索引热点 | game-service | 写冲突性能下降 | 分桶 |
| Token Session | gateway | Session 不共享 | Session 存储到 Redis |
| 私聊会话创建 | chat-service | 可能重复创建 | 数据库唯一约束 |
| 用户同步 | system-service | 可能重复同步 | 数据库唯一约束 |
| 好友关系添加 | system-service | 可能重复添加 | 数据库唯一约束 |
| WebSocket 连接管理 | game-service/chat-service | 并发注册可能重复 | 分布式锁 |
| Session 失效监听 | game-service/chat-service | Kafka 重复消费 | 去重机制 |

---

## 八、升级方案实施建议

### 8.1 第一阶段：P0 致命问题（2-3 天）

**必须修复，否则多实例部署不可用**

1. **AI 重复执行** → 分布式锁（1 天）
2. **倒计时重复恢复** → 分布式锁 + SCAN（0.5 天）
3. **SimpleBroker 跨实例** → 外部 broker 或房间亲和（1-1.5 天）

### 8.2 第二阶段：P1 严重问题（1 天）

**上线前必须修复**

4. **登录会话注册** → 分布式锁（0.5 天）
5. **CAS 失败回读** → 回读逻辑（0.5 天）
6. **好友关系校验** → 安全优先（0.5 天）

### 8.3 第三阶段：P2 中等问题（1-2 天）

**性能优化，可以延后**

7. **房间索引分桶** → 分桶逻辑（0.5 天）
8. **Session 存储** → Redis Session（0.5 天）
9. **数据库唯一约束** → 添加约束（0.5 天）
10. **Kafka 去重** → 去重机制（0.5 天）

### 8.4 接口幂等与重试策略（HTTP / WS / Kafka）

- **HTTP / WS 用户操作幂等**：  
  - 落子等对局操作依赖 Redis CAS（`expectedStep + expectedTurn`）保证**状态幂等**，同一步棋只会成功一次；  
  - 创建房间、好友申请、准备/开始/踢人等用户操作目前主要依赖状态校验和前端防抖，后续可以在 Gateway Filter / Spring 拦截器中统一读取前端传入的 `X-Idempotency-Key` / `requestId`，结合 Redis `SETNX` 记录“已处理请求”，实现**请求级幂等**，避免跨实例重试导致的重复提交；  
- **Kafka 会话事件幂等**：  
  - 生产端已开启幂等生产（`enable.idempotence=true` + `acks=all`），确保不会因重试产生重复消息；  
  - 消费端目前依赖“断开 WS 操作本身幂等”，重复执行不会出错但会浪费资源；上线前建议为事件增加 `eventId` 字段，并在 Redis 中记录已处理事件，实现真正的**消费端幂等**。  

---

## 九、总结

### 9.1 当前状态

**1.0 版本多实例部署状态**：❌ **不适合多实例部署**

**致命问题数量**：11 个（P0）

**必须修复后才能多实例部署**：11 个 P0 问题

### 9.2 修复后状态

**修复 P0 问题后**：✅ **可以多实例部署**（但需要完成所有 P0 修复）

**修复 P0 + P1 问题后**：✅ **可以安全多实例部署**

**修复所有问题后**：✅ **可以高性能多实例部署**

### 9.3 建议

1. **如果必须多实例部署**：至少修复 11 个 P0 问题（2-3 天工作量）
2. **如果只是 demo/单实例**：当前代码可以运行
3. **上线前**：必须完成所有 P0 和 P1 修复

---

---

## 十、快速参考表（按服务分类）

### Gateway 服务功能点

| 功能点 | 影响等级 | 是否需要改造 | 改造工作量 |
|--------|---------|------------|-----------|
| 登录/鉴权 | 🟠 严重 | ✅ 是 | 分布式锁（0.5 天） |
| 会话管理 | 🟠 严重 | ✅ 是 | 分布式锁（0.5 天） |
| Token 获取接口 | 🟡 中等 | ✅ 是 | Session 存储到 Redis（0.5 天） |

### Game-Service 服务功能点

| 功能点 | 影响等级 | 是否需要改造 | 改造工作量 |
|--------|---------|------------|-----------|
| 房间创建 | 🟡 中等 | ✅ 是 | 房间索引分桶（0.5 天） |
| 内存 Room 缓存 | 🟠 严重 | ✅ 是 | 缓存版本号或房间亲和（1 天） |
| 玩家落子 | 🟠 严重 | ✅ 是 | CAS 失败回读（0.5 天） |
| AI 执行 | 🔴 致命 | ✅ 是 | 分布式锁（0.5 天） |
| 倒计时恢复 | 🔴 致命 | ✅ 是 | 分布式锁 + SCAN（0.5 天） |
| Redis KEYS（倒计时） | 🔴 致命 | ✅ 是 | 改用 SCAN（0.5 天） |
| Redis KEYS（seatKey） | 🔴 致命 | ✅ 是 | 改用 SCAN（0.5 天） |
| WebSocket 广播（所有操作） | 🔴 致命 | ✅ 是 | 外部 broker 或房间亲和（1 天） |
| 房间加入/退出 | 🔴 致命 | ✅ 是 | 外部 broker 或房间亲和（1 天） |
| 座位绑定 | 🟡 中等 | ✅ 是 | deleteSeatKeys 改用 SCAN（0.5 天） |
| 重连恢复 | 🟡 中等 | ⚠️ 可选 | Session 存储到 Redis（0.5 天） |
| 认输/准备/开始/重开/踢人 | 🔴 致命 | ✅ 是 | 外部 broker 或房间亲和（1 天） |
| 倒计时 TICK/TIMEOUT | 🔴 致命 | ✅ 是 | 外部 broker 或房间亲和（1 天） |
| WebSocket 连接管理 | 🟡 中等 | ✅ 是 | 分布式锁（0.5 天） |
| Session 失效监听 | 🟡 中等 | ✅ 是 | 去重机制（0.5 天） |

### Chat-Service 服务功能点

| 功能点 | 影响等级 | 是否需要改造 | 改造工作量 |
|--------|---------|------------|-----------|
| 大厅聊天 | 🔴 致命 | ✅ 是 | 外部 broker（1 天） |
| 房间聊天 | 🔴 致命 | ✅ 是 | 外部 broker（1 天） |
| 私聊消息 | 🔴 致命 | ✅ 是 | 外部 broker（1 天） |
| 私聊会话创建 | 🟡 中等 | ✅ 是 | 数据库唯一约束（0.5 天） |
| 好友关系校验 | 🟠 严重 | ✅ 是 | 安全优先（0.5 天） |
| WebSocket 连接管理 | 🟡 中等 | ✅ 是 | 分布式锁（0.5 天） |
| Session 失效监听 | 🟡 中等 | ✅ 是 | 去重机制（0.5 天） |

### System-Service 服务功能点

| 功能点 | 影响等级 | 是否需要改造 | 改造工作量 |
|--------|---------|------------|-----------|
| 用户同步 | 🟡 中等 | ✅ 是 | 数据库唯一约束（0.5 天） |
| 好友关系管理 | 🟡 中等 | ✅ 是 | 数据库唯一约束（0.5 天） |
| 会话监控 | 🟡 中等 | ✅ 是 | SessionRegistry KEYS 改 SCAN（0.5 天） |

### 公共库功能点

| 功能点 | 影响等级 | 是否需要改造 | 改造工作量 |
|--------|---------|------------|-----------|
| SessionRegistry 查询（KEYS） | 🟡 中等 | ✅ 是 | 改用 SCAN（0.5 天） |
| Kafka 事件消费 | 🟡 中等 | ✅ 是 | 去重机制（0.5 天） |

---

## 十一、改造工作量汇总

### 按优先级汇总

| 优先级 | 问题数量 | 总工作量 | 必须完成时间 |
|--------|---------|---------|------------|
| **P0（致命）** | 11 个 | 2-3 天 | 多实例部署前 |
| **P1（严重）** | 4 个 | 1-1.5 天 | 上线前 |
| **P2（中等）** | 9 个 | 1-2 天 | 性能优化阶段 |

### 按服务汇总

| 服务 | 需要改造的功能点 | 总工作量 |
|------|-----------------|---------|
| **Gateway** | 3 个 | 1 天 |
| **Game-Service** | 18 个 | 2-3 天 |
| **Chat-Service** | 8 个 | 2-3 天 |
| **System-Service** | 3 个 | 0.5-1 天 |
| **公共库** | 2 个 | 1 天 |

### 改造后状态

**修复 P0 问题后**：✅ **可以多实例部署**（但需要完成所有 11 个 P0 修复）

**修复 P0 + P1 问题后**：✅ **可以安全多实例部署**

**修复所有问题后**：✅ **可以高性能多实例部署**

---

## 十二、详细改造方案与代码示例

### 12.1 分布式锁工具类（新增）

**文件位置**：`game-service/src/main/java/com/gamehub/gameservice/infrastructure/lock/DistributedLockUtil.java`

```java
package com.gamehub.gameservice.infrastructure.lock;

import lombok.RequiredArgsConstructor;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.data.redis.core.script.DefaultRedisScript;
import org.springframework.stereotype.Component;
import java.time.Duration;
import java.util.Collections;

/**
 * 分布式锁工具类
 * 用于多实例部署下的并发控制
 */
@Component
@RequiredArgsConstructor
public class DistributedLockUtil {
    private final RedisTemplate<String, Object> redis;
    
    @Value("${instance.id:${spring.application.name}-${random.value}}")
    private String instanceId;
    
    /**
     * 尝试获取分布式锁
     * @param lockKey 锁的 key
     * @param ttlSeconds TTL（秒）
     * @return true 表示获取成功
     */
    public boolean tryLock(String lockKey, int ttlSeconds) {
        Boolean acquired = redis.opsForValue().setIfAbsent(
            lockKey, instanceId, Duration.ofSeconds(ttlSeconds));
        return Boolean.TRUE.equals(acquired);
    }
    
    /**
     * 释放锁（只有持有者才能释放）
     * 使用 Lua 脚本保证原子性：检查 + 删除
     */
    public void releaseLock(String lockKey) {
        // 使用 Lua 脚本保证原子性：只有持有者才能释放
        String script = "if redis.call('get', KEYS[1]) == ARGV[1] then " +
                        "return redis.call('del', KEYS[1]) " +
                        "else return 0 end";
        redis.execute(new DefaultRedisScript<>(script, Long.class), 
                     Collections.singletonList(lockKey), instanceId);
    }
}
```

**改动量**：新增文件，~60 行代码

**注意事项**：
- 释放锁使用 Lua 脚本保证原子性，避免竞态条件
- 锁的 TTL 应该大于任务执行时间，避免任务未完成锁已过期
- 如果任务执行时间不确定，可以考虑"锁续期"机制（watchdog）

---

### 12.2 AI 重复执行修复（详细代码）

**文件位置**：`GomokuWsController.java`

**改动前**：
```java
private void maybeScheduleAi(String roomId, GomokuState state, String gameIdAtSchedule) {
    if (state.over() || gomokuService.getMode(roomId) != Mode.PVE) return;
    if (state.current() != gomokuService.getAiPiece(roomId)) return;
    
    // 取消该房间之前的AI任务
    ScheduledFuture<?> old = pendingAi.remove(roomId);
    if (old != null) old.cancel(false);
    
    // 调度 AI 任务
    ScheduledFuture<?> fut = aiScheduler.schedule(
        () -> {
            runAiTurn(roomId, gameIdAtSchedule, ai);
            pendingAi.remove(roomId);
        }, delay, TimeUnit.MILLISECONDS);
    
    pendingAi.put(roomId, fut);
}
```

**改动后**：
```java
private final DistributedLockUtil lockUtil;  // 新增依赖

private void maybeScheduleAi(String roomId, GomokuState state, String gameIdAtSchedule) {
    if (state.over() || gomokuService.getMode(roomId) != Mode.PVE) return;
    if (state.current() != gomokuService.getAiPiece(roomId)) return;
    
    // ✅ 新增：分布式锁检查
    String lockKey = "ai:lock:" + roomId;
    if (!lockUtil.tryLock(lockKey, 5)) {
        log.debug("AI 任务已在其他实例执行，跳过: roomId={}", roomId);
        return;
    }
    
    try {
        // 取消该房间之前的AI任务
        ScheduledFuture<?> old = pendingAi.remove(roomId);
        if (old != null) old.cancel(false);
        
        // 调度 AI 任务
        ScheduledFuture<?> fut = aiScheduler.schedule(
            () -> {
                try {
                    runAiTurn(roomId, gameIdAtSchedule, ai);
                } finally {
                    // ✅ 新增：执行完成后释放锁
                    lockUtil.releaseLock(lockKey);
                    pendingAi.remove(roomId);
                }
            }, delay, TimeUnit.MILLISECONDS);
        
        pendingAi.put(roomId, fut);
    } catch (Exception e) {
        // ✅ 新增：异常时释放锁
        lockUtil.releaseLock(lockKey);
        throw e;
    }
}
```

**改动量**：~20 行代码改动

---

### 12.3 倒计时恢复修复（详细代码）

**文件位置**：`CountdownSchedulerImpl.java`

**改动前**：
```java
@Override
public int restoreAllActive(TimeoutHandler onTimeout) {
    Set<String> keys = redis.keys(stateKey("*"));  // ⚠️ 阻塞操作
    // ... 恢复逻辑 ...
}
```

**改动后**：
```java
@Override
public int restoreAllActive(TimeoutHandler onTimeout) {
    // ✅ 新增：分布式锁，只有一个实例执行恢复
    String lockKey = "countdown:restore:lock";
    Boolean acquired = redis.opsForValue().setIfAbsent(
        lockKey, nodeId, Duration.ofSeconds(30));
    
    if (!Boolean.TRUE.equals(acquired)) {
        log.info("其他实例正在恢复倒计时，跳过: nodeId={}", nodeId);
        return 0;
    }
    
    try {
        // ✅ 改造：KEYS → SCAN
        Set<String> keys = new HashSet<>();
        String cursor = "0";
        String pattern = stateKey("*");
        
        do {
            ScanOptions options = ScanOptions.scanOptions()
                .match(pattern)
                .count(100)  // 每次扫描 100 个 key
                .build();
            Cursor<String> scanCursor = redis.scan(options);
            
            while (scanCursor.hasNext()) {
                keys.add(scanCursor.next());
            }
            cursor = scanCursor.getCursor();
        } while (!"0".equals(cursor));
        
        // ... 后续恢复逻辑不变 ...
        return restored;
    } finally {
        // ✅ 新增：恢复完成后释放锁
        redis.delete(lockKey);
    }
}
```

**改动量**：~35 行代码改动（分布式锁 + SCAN）

---

### 12.4 SimpleBroker 改造（详细代码）

**文件位置**：`WebSocketStompConfig.java`

**方案 A：Redis PubSub（推荐）**

**改动前**：
```java
@Override
public void configureMessageBroker(MessageBrokerRegistry registry) {
    registry.enableSimpleBroker("/topic", "/queue");
    registry.setApplicationDestinationPrefixes("/app");
    registry.setUserDestinationPrefix("/user");
}
```

**改动后**：
```java
@Override
public void configureMessageBroker(MessageBrokerRegistry registry) {
    // ✅ 改造：SimpleBroker → StompBrokerRelay (Redis)
    registry.enableStompBrokerRelay("/topic", "/queue")
        .setRelayHost("${redis.host:localhost}")
        .setRelayPort(${redis.port:6379})
        .setClientLogin("guest")
        .setClientPasscode("guest")
        .setSystemLogin("guest")
        .setSystemPasscode("guest");
    
    registry.setApplicationDestinationPrefixes("/app");
    registry.setUserDestinationPrefix("/user");
}
```

**方案 B：房间亲和（改动中等，但需要网关配合）**

在网关层添加路由过滤器：
```java
// gateway/src/main/java/com/gamehub/gateway/filter/RoomAffinityFilter.java
@Component
public class RoomAffinityFilter implements GatewayFilter {
    @Override
    public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) {
        // 从请求中提取 roomId
        String roomId = extractRoomId(exchange);
        if (roomId != null) {
            // 按 roomId 做一致性哈希，路由到固定实例
            String targetInstance = consistentHash(roomId, availableInstances);
            // 修改请求的目标实例
            exchange.getRequest().mutate().uri(URI.create(targetInstance));
        }
        return chain.filter(exchange);
    }
}
```

**改动量**：
- 方案 A：~10 行代码改动
- 方案 B：~50 行代码（网关 + 配置）

---

### 12.5 CAS 失败后回读（详细代码）

**文件位置**：`GomokuWsController.java`

**改动前**：
```java
@MessageMapping("/gomoku.place")
public void place(PlaceCmd cmd, SimpMessageHeaderAccessor sha) {
    // 1. 内存状态变更
    GomokuState state = gomokuService.place(roomId, x, y, caller);
    
    // 2. Redis CAS 更新
    boolean success = gameStateRepository.updateAtomically(...);
    // ⚠️ 如果失败，内存状态已脏，但后续逻辑可能用到
}
```

**改动后**：
```java
@MessageMapping("/gomoku.place")
public void place(PlaceCmd cmd, SimpMessageHeaderAccessor sha) {
    try {
        // 1. 内存状态变更
        GomokuState state = gomokuService.place(roomId, x, y, caller);
        
        // 2. Redis CAS 更新
        int expectedStep = computeExpectedStep(state.board()) - 1;
        char expectedTurn = state.current() == 'X' ? 'O' : 'X';
        GameStateRecord rec = buildRecord(state, roomId, gameIdAtSchedule, expectedStep + 1);
        long nextDeadlineMs = state.over() ? 0L : System.currentTimeMillis() + turnSeconds * 1000L;
        
        // ✅ 新增：CAS 失败后回读
        boolean success = gameStateRepository.updateAtomically(
            roomId, gameIdAtSchedule, expectedStep, expectedTurn, rec, nextDeadlineMs);
        
        if (!success) {
            // CAS 失败，回读 Redis 刷新内存
            log.warn("CAS 失败，回读 Redis 刷新内存: roomId={}", roomId);
            GomokuState freshState = gameStateRepository.load(roomId);
            if (freshState != null) {
                gomokuService.refreshState(roomId, freshState);
            }
            throw new ConcurrentModificationException("并发冲突，请重试");
        }
        
        // ... 后续逻辑 ...
    } catch (Exception ex) {
        // ... 异常处理 ...
    }
}
```

**改动量**：~15 行代码改动 + 新增 `GomokuService.refreshState()` 方法（~10 行）

---

## 十三、改造步骤详细计划

### 第一阶段：P0 核心问题（1-2 天）

#### Day 1 上午：AI 重复执行修复
1. 新增 `DistributedLockUtil` 工具类
2. 修改 `GomokuWsController.maybeScheduleAi()`
3. 单实例测试（验证锁逻辑正确）

#### Day 1 下午：倒计时重复恢复修复
1. 修改 `CountdownSchedulerImpl.restoreAllActive()`
2. 加分布式锁 + 改 SCAN
3. 单实例测试（验证 SCAN 逻辑正确）

#### Day 2 上午：SimpleBroker 改造
1. 选择方案（推荐房间亲和，改动小）
2. 实现路由逻辑（网关层或配置层）
3. 单实例测试（验证路由正确）

#### Day 2 下午：多实例联调测试
1. 搭建 2-3 个实例环境
2. 测试 AI 不重复、倒计时不重复、广播正常
3. 压力测试（并发落子、并发登录）

### 第二阶段：P1 优化（0.5-1 天）

#### Day 3 上午：CAS 失败回读
1. 修改 `GomokuWsController.place()`
2. 新增 `GomokuService.refreshState()` 方法
3. 测试并发落子场景（验证回读逻辑）

#### Day 3 下午：SessionRegistry 优化
1. 修改 `SessionRegistry.registerLoginSessionEnforceSingle()`
2. 测试并发登录场景（验证单设备登录）

### 第三阶段：P2 性能优化（1-2 天，可选）

#### Day 4：其他优化
1. 房间索引分桶
2. Session 存储到 Redis
3. 数据库唯一约束
4. Kafka 去重机制

---

## 十四、改造前后架构对比

### 改造前（单实例）

```
┌─────────────┐
│   Gateway   │
└──────┬──────┘
       │
       ▼
┌─────────────────────────────┐
│   Game-Service (单实例)     │
│   ├─ pendingAi (内存)       │
│   ├─ SimpleBroker (内存)    │
│   └─ restoreAllActive (无锁) │
└─────────────────────────────┘
       │
       ▼
┌─────────────┐
│    Redis    │
└─────────────┘
```

### 改造后（多实例）

```
┌─────────────┐
│   Gateway   │
│  (路由/亲和) │
└──────┬──────┘
       │
       ├──────────────────────┐
       ▼                      ▼
┌──────────────────┐  ┌──────────────────┐
│ Game-Service (1)  │  │ Game-Service (2) │
│ ├─ DistributedLock│  │ ├─ DistributedLock│
│ ├─ Redis PubSub  │  │ ├─ Redis PubSub  │
│ └─ SCAN 恢复     │  │ └─ SCAN 恢复     │
└────────┬─────────┘  └────────┬─────────┘
         │                      │
         └──────────┬───────────┘
                    ▼
            ┌─────────────┐
            │    Redis    │
            │  (共享存储)  │
            └─────────────┘
```

---

## 十五、风险评估与回滚方案

### 改造风险

| 风险 | 等级 | 缓解措施 |
|------|------|---------|
| **分布式锁死锁** | 低 | TTL 自动释放，finally 确保释放 |
| **锁竞争性能** | 低 | 锁粒度细（按 roomId），竞争概率低 |
| **SCAN 漏数据** | 低 | 恢复场景可接受最终一致 |
| **外部 broker 故障** | 中 | 房间亲和方案不依赖外部 broker |

### 回滚方案

如果改造后出现问题，可以：

1. **快速回滚**：移除分布式锁逻辑，恢复单实例部署
2. **渐进式回滚**：先修复 AI 和倒计时，SimpleBroker 保持房间亲和
3. **部分回滚**：只回滚有问题的功能点，保留其他修复

---

**分析完成时间**：2025-12-17

